{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import ParserUDpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(\"They go.\")\n",
    "df = parser.conllu2df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepRel</th>\n",
       "      <th>Deps</th>\n",
       "      <th>Feats</th>\n",
       "      <th>Form</th>\n",
       "      <th>Head</th>\n",
       "      <th>Id</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Misc</th>\n",
       "      <th>UPosTag</th>\n",
       "      <th>XPosTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>Number=Plur|Person=3|PronType=Prs</td>\n",
       "      <td>They</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>they</td>\n",
       "      <td>_</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>root</td>\n",
       "      <td>_</td>\n",
       "      <td>Mood=Ind|Number=Plur|Tense=Pres|VerbForm=Fin</td>\n",
       "      <td>go</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>go</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>VERB</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>punct</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>.</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  DepRel Deps                                         Feats  Form  Head   Id  \\\n",
       "0  nsubj    _             Number=Plur|Person=3|PronType=Prs  They   2.0  1.0   \n",
       "1   root    _  Mood=Ind|Number=Plur|Tense=Pres|VerbForm=Fin    go   0.0  2.0   \n",
       "2  punct    _                                             _     .   2.0  3.0   \n",
       "\n",
       "  Lemma           Misc UPosTag XPosTag  \n",
       "0  they              _    PRON      PE  \n",
       "1    go  SpaceAfter=No    VERB       V  \n",
       "2     .  SpaceAfter=No   PUNCT      FS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dolphins love little dolphins.\n",
    "forms = 3: 'love', 'dolphins', 'little'\n",
    "lemmas = 3: 'love', 'dolphin', 'little'\n",
    "tokens = 4: 'dolphins', 'love', 'dolphins', 'little'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверить на punct\n",
    "# сначала спеллчеккер\n",
    "# разобраться с D\n",
    "# https://github.com/kristopherkyle/lexical_diversity отсюда взять критерии\n",
    "# нужно бы чтобы токены и лексемы были все в нижнем регистре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "\n",
    "open_class = [\"NOUN\", \"VERB\", \"ADV\", \"ADJ\"]\n",
    "with open('lists.json') as data_file:\n",
    "    lists = json.load(data_file)\n",
    "fivetfrequentCOCA = lists['5000frequentCOCA']\n",
    "frequentverbsCOCAfromfivet = lists['frequentverbsCOCAfrom5000']\n",
    "uwl = lists['UWL']\n",
    "\n",
    "class LexicalComplexity:\n",
    "    \"\"\"Returns values of lexical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    def get_verb_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'VERB']['Lemma']\n",
    "    \n",
    "    def get_noun_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'NOUN']['Lemma']\n",
    "\n",
    "    def get_adj_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADJ']['Lemma']\n",
    "    \n",
    "    def get_adv_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADV']['Lemma']\n",
    "    \n",
    "    def get_lex_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'].isin(open_class)]['Lemma']\n",
    "    \n",
    "    def get_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Lemma']\n",
    "    \n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "    \n",
    "    def safe_divide(self, numerator, denominator):\n",
    "        if denominator == 0 or denominator == 0.0:\n",
    "            index = 0\n",
    "        else: index = numerator/denominator\n",
    "        return index\n",
    "\n",
    "    def division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def corrected_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(2*len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def root_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def squared_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)**2/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def log_division(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))/math.log(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def uber(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))**2/math.log(len(set(list2))/len(list1))\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def density(self, punct=False):\n",
    "        \"\"\"\n",
    "        number of lexical tokens/number of tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        lemmas = self.get_lemmas()\n",
    "        return self.division(lex_lemmas, lemmas)\n",
    "    \n",
    "    def LS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated lexical tokens/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        soph_lex_lemmas = [i for i in lex_lemmas if i not in fivetfrequentCOCA]\n",
    "        return self.division(soph_lex_lemmas, lex_lemmas)\n",
    "    \n",
    "    def VS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated verb lemmas/number of verb tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = self.get_verb_lemmas()\n",
    "        soph_verbs = set([i for i in verb_lemmas if i not in frequentverbsCOCAfromfivet])\n",
    "        VSI = self.division(soph_verbs, verb_lemmas)\n",
    "        VSII = self.corrected_division(soph_verbs, verb_lemmas)\n",
    "        VSIII = self.squared_division(soph_verbs, verb_lemmas)\n",
    "        return VSI, VSII, VSIII\n",
    "\n",
    "    def LFP(self):\n",
    "        \"\"\"\n",
    "        Lexical Frequency Profile is the proportion of tokens:\n",
    "        first - 1000 most frequent words\n",
    "        second list - the second 1000\n",
    "        third - University Word List (Xue & Nation 1989)\n",
    "        none - list of those that are not in these lists\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        first = [i for i in lemmas if i in fivetfrequentCOCA[0:1000]]\n",
    "        second = [i for i in lemmas if i in fivetfrequentCOCA[1000:2000]]\n",
    "        third = [i for i in lemmas if i in uwl]\n",
    "        first_procent = self.division(first, lemmas)\n",
    "        second_procent = self.division(second, lemmas)\n",
    "        third_procent = self.division(third, lemmas)\n",
    "        none = 1 - (first_procent + second_procent + third_procent)\n",
    "        return first_procent, second_procent , third_procent, none\n",
    "    \n",
    "    def NDW(self):\n",
    "        \"\"\"\n",
    "        number of lemmas\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        return len(set(lemmas))\n",
    "    \n",
    "    def TTR(self):\n",
    "        \"\"\"\n",
    "        number of lemmas/number of tokens\n",
    "        \"\"\"\n",
    "        lemmas = set(self.get_lemmas())\n",
    "        tokens = self.get_lemmas()\n",
    "        TTR = self.division(lemmas, tokens)\n",
    "        CTTR = self.corrected_division(lemmas, tokens)\n",
    "        RTTR = self.root_division(lemmas, tokens)\n",
    "        LogTTR = self.log_division(lemmas, tokens)\n",
    "        Uber = self.uber(lemmas, tokens)\n",
    "        return TTR, CTTR, RTTR, LogTTR, Uber\n",
    "\n",
    "    def choose(self, n, k):\n",
    "        \"\"\"\n",
    "        Calculates binomial coefficients\n",
    "        \"\"\"\n",
    "        if 0 <= k <= n:\n",
    "            ntok = 1\n",
    "            ktok = 1\n",
    "            for t in range(1, min(k, n - k) + 1):\n",
    "                ntok *= n\n",
    "                ktok *= t\n",
    "                n -= 1\n",
    "            return ntok // ktok\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def hyper(self, successes, sample_size, population_size, freq):\n",
    "        \"\"\"\n",
    "        Calculates hypergeometric distribution\n",
    "        \"\"\"\n",
    "        # probability a word will occur at least once in a sample of a particular size\n",
    "        try:\n",
    "            prob_1 = 1.0 - (float((self.choose(freq, successes) * \n",
    "                                   self.choose((population_size - freq),\n",
    "                                               (sample_size - successes)))) /\n",
    "                            float(self.choose(population_size, sample_size)))\n",
    "            prob_1 = prob_1 * (1/sample_size)\n",
    "        except ZeroDivisionError:\n",
    "            prob_1 = 0\n",
    "        return prob_1\n",
    "    \n",
    "    def D(self):\n",
    "        prob_sum = 0.0\n",
    "        tokens = self.get_forms()\n",
    "        num_tokens = len(tokens)\n",
    "        types_list = list(set(tokens))\n",
    "        frequency_dict = collections.Counter(tokens)\n",
    "\n",
    "        for items in types_list:\n",
    "            # random sample is 42 items in length\n",
    "            prob = self.hyper(0, 42, num_tokens, frequency_dict[items])\n",
    "            prob_sum += prob\n",
    "\n",
    "        return prob_sum\n",
    "\n",
    "    def LV(self):\n",
    "        \"\"\"\n",
    "        number of lexical lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = set(self.get_lex_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return len(lex_lemmas)/len(lex_tokens)\n",
    "    \n",
    "    def VV(self):\n",
    "        \"\"\"\n",
    "        VVI: number of verb lemmas/number of verb tokens\n",
    "        VVII: number of verb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = set(self.get_verb_lemmas())\n",
    "        verb_tokens = self.get_verb_lemmas()\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        VVI = self.division(verb_lemmas, verb_tokens)\n",
    "        SVVI = self.squared_division(verb_lemmas, verb_tokens)\n",
    "        CVVI = self.corrected_division(verb_lemmas, verb_tokens)\n",
    "        VVII = self.division(verb_lemmas, lex_tokens)\n",
    "        return VVI, SVVI, CVVI, VVII\n",
    "        \n",
    "    def NV(self):\n",
    "        \"\"\"\n",
    "        number of noun lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        noun_lemmas = set(self.get_noun_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(noun_lemmas, lex_tokens)\n",
    "\n",
    "    def AdjV(self):\n",
    "        \"\"\"\n",
    "        number of adjective lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adj_lemmas = set(self.get_adj_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(adj_lemmas, lex_tokens)\n",
    "    \n",
    "    def AdvV(self):\n",
    "        \"\"\"\n",
    "        number of adverb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adv_lemmas = set(self.get_adv_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(adv_lemmas, lex_tokens)\n",
    "    \n",
    "    def ModV(self):\n",
    "        return self.AdjV() + self.AdvV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The graph contains information about money people spend on petrol. The research was done in the USA and the UK. Three classes were compared: the poorest, the richest and middle-income people.\\nResults in two countries are absolutely the opposite. The UK-line gradually goes up, and reaches the peak on the point of 4 per cent. While the USA-line declines from the point of 5,3 per cent to 2,2 per cent. It means that the biggest amount of money is spent in the USA by poorest people. The same class in the UK spends only 0,5 per cent of the income. The difference in part of rich people is modest - about 1 per cent.\\nOverall, people from the USA spend bigger part of their income on petrol.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/irene/Desktop/Диплом/new_data/1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "LC = LexicalComplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lex_comp = {'density': LC.density(), 'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "                 'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "                 'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "                 'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "                 'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.D(), \n",
    "                 'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "                 'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "                 'ModV': LC.ModV()}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AdjV': 0.06779661016949153,\n",
       " 'AdvV': 0.06779661016949153,\n",
       " 'CTTR': 3.9075193515826885,\n",
       " 'CVVI': 1.7888543819998317,\n",
       " 'D': 0.7267636142307924,\n",
       " 'LFP_first': 0.6326530612244898,\n",
       " 'LFP_none': 0.23809523809523803,\n",
       " 'LFP_second': 0.08843537414965986,\n",
       " 'LFP_third': 0.04081632653061224,\n",
       " 'LS': 0.2542372881355932,\n",
       " 'LV': 0.6440677966101694,\n",
       " 'LogTTR': 0.8425507300770982,\n",
       " 'ModV': 0.13559322033898305,\n",
       " 'NDW': 67,\n",
       " 'NV': 0.3898305084745763,\n",
       " 'RTTR': 5.526066862243561,\n",
       " 'SVVI': 6.4,\n",
       " 'TTR': 0.4557823129251701,\n",
       " 'Uber': 0,\n",
       " 'VSI': 0.0,\n",
       " 'VSII': 0.0,\n",
       " 'VSIII': 0.0,\n",
       " 'VVI': 0.8,\n",
       " 'VVII': 0.13559322033898305,\n",
       " 'density': 0.4013605442176871}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_lex_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше выделять суффиксы\n",
    "# научиться выделять приставки\n",
    "# MCI: went -> ed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "go\t\tinf\t\t\t\t\t\tVerbForm=Inf\n",
    "going\tgerund\t\t\t\t\tVerbForm=Ger\n",
    "goes\t3rd singular present\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
    "go\t\tpresent\t\t\t\t\tMood=Ind|Number=Plur|Tense=Pres|VerbForm=Fin\n",
    "gone\tpast participle\t\t\tTense=Past|VerbForm=Part\n",
    "went\tsimple past\t\t\t\tMood=Ind|Person=3|Tense=Past|VerbForm=Fin\n",
    "\n",
    "df[df['Feats'].str.match('.*VerbForm=Inf.*')]\n",
    "df[df['Feats'].str.match('.*VerbForm=Ger.*')]\n",
    "df[df['Feats'].str.match('.*Mood=Ind.+Number=Sing.+Person=3.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "df[df['Feats'].str.match('.*Mood=Ind.+Number=Plur.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "df[df['Feats'].str.match('.*Tense=Past.+VerbForm=Part.*')]\n",
    "df[df['Feats'].str.match('.*Mood=Ind.+Person=3.+Tense=Past.+VerbForm=Fin.*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "with open('suffixes.json') as data_file:\n",
    "    suffixes_levels = json.load(data_file)\n",
    "\n",
    "class MorphologicalComplexity:\n",
    "    \"\"\"Returns values of morphological criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "   \n",
    "    def get_inf(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*VerbForm=Inf.*')]\n",
    "    \n",
    "    def get_gerund(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*VerbForm=Ger.*')]\n",
    "    \n",
    "    def get_pres_sg(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Mood=Ind.+Number=Sing.+Person=3.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "    \n",
    "    def get_pres_pl(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Mood=Ind.+Number=Plur.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "    \n",
    "    def get_part(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Tense=Past.+VerbForm=Part.*')]\n",
    "    \n",
    "    def get_past(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Mood=Ind.+Person=3.+Tense=Past.+VerbForm=Fin.*')]     \n",
    "        \n",
    "    def get_verb_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'VERB']['Form']\n",
    "    \n",
    "    def get_verb_feats(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.contains('VerbForm=')]['Feats']\n",
    "    \n",
    "    def get_aux(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'AUX']['Form']\n",
    "    \n",
    "    def division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def one_random_list(self, l, length):\n",
    "        result = []\n",
    "        for i in range(length):\n",
    "            random_element = random.choice(l)\n",
    "            l.remove(random_element)\n",
    "            result.append(random_element)\n",
    "        return result, l\n",
    "\n",
    "    def two_random_lists(self, l, length=10):\n",
    "        list1, list2 = [], []\n",
    "        if len(l) < length*2:\n",
    "            return self.two_random_lists(l, length=length-1)\n",
    "        else:\n",
    "            list1, l = self.one_random_list(l, length)\n",
    "            list2, l = self.one_random_list(l, length)\n",
    "            return list1, list2\n",
    "        \n",
    "    def num_uniques(self, l):\n",
    "        counter=collections.Counter(l)\n",
    "        return list(counter.values()).count(1)\n",
    "\n",
    "    def get_suffix(self, word):\n",
    "        root = porter_stemmer.stem(word)\n",
    "        suffix = word[len(root):]\n",
    "        return suffix\n",
    "    \n",
    "    def get_suffixes(self):\n",
    "        forms = self.get_forms()\n",
    "        suffixes = [self.get_suffix(word) for word in forms]\n",
    "        return list(filter(lambda s: s != \"\", suffixes))\n",
    "        \n",
    "    def derivational_suffixation(self):\n",
    "        \"\"\"\n",
    "        number of suffixes on n's level/number of suffixes\n",
    "        \"\"\"\n",
    "        suffixes = self.get_suffixes()\n",
    "        level3_suffixes = [i for i in suffixes if i in suffixes_levels[\"level3\"]]\n",
    "        level4_suffixes = [i for i in suffixes if i in suffixes_levels[\"level4\"]]\n",
    "        level5_suffixes = [i for i in suffixes if i in suffixes_levels[\"level5\"]]\n",
    "        level6_suffixes = [i for i in suffixes if i in suffixes_levels[\"level6\"]]\n",
    "        der_suff3 = self.division(level3_suffixes, suffixes)\n",
    "        der_suff4 = self.division(level4_suffixes, suffixes)\n",
    "        der_suff5 = self.division(level5_suffixes, suffixes)\n",
    "        der_suff6 = self.division(level6_suffixes, suffixes)\n",
    "        return der_suff3, der_suff4, der_suff5, der_suff6\n",
    "    \n",
    "    def MCI(self):\n",
    "        \"\"\"\n",
    "        MCI represents the average inflectional diversity for the parts of speech in the sample\n",
    "        \"\"\"\n",
    "        verb_forms = self.get_verb_forms()\n",
    "        suff_verb = [self.get_suffix(verb) for verb in verb_forms]\n",
    "        list1, list2 = self.two_random_lists(suff_verb)\n",
    "        diversity1=len(set(list1))\n",
    "        diversity2=len(set(list2))\n",
    "        mean_diversity = (diversity1+diversity2)/2\n",
    "        num_uni = self.num_uniques(list1+list2)\n",
    "        IUV = num_uni/2\n",
    "        MCI = mean_diversity + IUV/2 - 1\n",
    "        return MCI\n",
    "    \n",
    "    def freq_finite_forms(self):\n",
    "        \"\"\"\n",
    "        frequency of tensed(finite) forms\n",
    "        \"\"\"\n",
    "        verb_feats = self.get_verb_feats()\n",
    "        finite_forms = [word for word in verb_feats if \"VerbForm=Fin\" in word]\n",
    "        return self.division(finite_forms, verb_feats)\n",
    "    \n",
    "    def freq_aux(self):\n",
    "        \"\"\"\n",
    "        frequency of modals(auxilaries)\n",
    "        \"\"\"\n",
    "        verb_feats = self.get_verb_feats()\n",
    "        aux = self.get_aux()\n",
    "        return self.division(aux, verb_feats)\n",
    "\n",
    "    def num_verb_forms(self):\n",
    "        \"\"\"\n",
    "        number of different verb forms: \n",
    "        infinitives, gerunds, present singular, present plural, past participle, past simple\n",
    "        \"\"\"\n",
    "        inf = self.get_inf()\n",
    "        gerund = self.get_gerund()\n",
    "        pres_sg = self.get_pres_sg()\n",
    "        pres_pl = self.get_pres_pl()\n",
    "        part = self.get_part()\n",
    "        past = self.get_past()\n",
    "        return len(inf), len(gerund), len(pres_sg), len(pres_pl), len(part), len(past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The graph contains information about money people spend on petrol. The research was done in the USA and the UK. Three classes were compared: the poorest, the richest and middle-income people.\\nResults in two countries are absolutely the opposite. The UK-line gradually goes up, and reaches the peak on the point of 4 per cent. While the USA-line declines from the point of 5,3 per cent to 2,2 per cent. It means that the biggest amount of money is spent in the USA by poorest people. The same class in the UK spends only 0,5 per cent of the income. The difference in part of rich people is modest - about 1 per cent.\\nOverall, people from the USA spend bigger part of their income on petrol.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "MC = MorphologicalComplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_morph_comp = {'der_suff_level3': MC.derivational_suffixation()[0], \n",
    "                   'der_suff_level4': MC.derivational_suffixation()[1], \n",
    "                   'der_suff_level5': MC.derivational_suffixation()[2], \n",
    "                   'der_suff_level6': MC.derivational_suffixation()[3],\n",
    "                   'MCI': MC.MCI(), 'freq_finite_forms': MC.freq_finite_forms(), \n",
    "                   'freq_aux': MC.freq_aux(), 'num_inf': MC.num_verb_forms()[0], \n",
    "                   'num_inf': MC.num_verb_forms()[0], 'num_gerund': MC.num_verb_forms()[1], \n",
    "                   'num_pres_sg': MC.num_verb_forms()[2], 'num_pres_pl': MC.num_verb_forms()[3], \n",
    "                   'num_part': MC.num_verb_forms()[4], 'num_past': MC.num_verb_forms()[5]}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MCI': 2.75,\n",
       " 'der_suff_level3': 0.038461538461538464,\n",
       " 'der_suff_level4': 0.038461538461538464,\n",
       " 'der_suff_level5': 0.07692307692307693,\n",
       " 'der_suff_level6': 0.0,\n",
       " 'freq_aux': 0.3333333333333333,\n",
       " 'freq_finite_forms': 0.8,\n",
       " 'num_gerund': 0,\n",
       " 'num_inf': 0,\n",
       " 'num_part': 3,\n",
       " 'num_past': 1,\n",
       " 'num_pres_pl': 2,\n",
       " 'num_pres_sg': 8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_morph_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of tokens, minimum/maximum/average depth of the sentence, number of relative clauses, number of adverbial clauses, number of modifier clauses, number of sentences, number of clauses, number of T-units, number of complex T-units, number of coordinate phrases, number of noun phrases (possessive structures,  prepositional phrases, infinitives or gerunds in the position of object or subject, phrases like “adjective + noun”, “participle + noun”, “noun + infinitive”), number of complex noun phrases, number of verb phrases, Coordination Index, variety of constructions, average number of tokens before the root of the sentence, mean length of the sentence, mean length of the clause, number of clauses per sentence, number of clauses per T-unit, number of dependent clauses per clause, number of dependent clauses per T-unit, number of coordinate phrases per clause, number of T-units per sentence, number of possessive structures per sentence,  number of prepositional phrases per sentence, number of infinitives or gerunds in the position of object or subject per sentence, number of phrases like “adjective + noun”, “participle + noun”, “noun + infinitive” per sentence,  number of verb phrases per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# дописать класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class SyntacticComplexity:\n",
    "    \"\"\"Returns values of syntactical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.get_forms())\n",
    "\n",
    "    def order_head(self, sent):\n",
    "        _id = sent['Id']\n",
    "        _head = sent['Head']\n",
    "        _form = sent['Form']\n",
    "        return(list(zip(_id, _head)))\n",
    "    \n",
    "    def get_dep_rel(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['DepRel']\n",
    "\n",
    "    def find_root(self, order_head_lst):\n",
    "        for every_order_head in order_head_lst:\n",
    "            if every_order_head[1] == 0:\n",
    "                root = every_order_head\n",
    "        return root\n",
    "\n",
    "    def root_children(self, sent):\n",
    "        order_head_lst = self.order_head(sent)\n",
    "        root = self.find_root(order_head_lst)\n",
    "        chains = []\n",
    "        for every_order_head in order_head_lst:\n",
    "            if every_order_head[1] == root[0]:\n",
    "                chains.append([root[0], every_order_head[0]])\n",
    "        return chains, order_head_lst\n",
    "\n",
    "    def chains_heads(self, chains, order_head_lst):\n",
    "        length_chains = len(chains)\n",
    "        i = 0\n",
    "        for chain in chains:\n",
    "            if i < length_chains:\n",
    "                heads = []\n",
    "                if 'stop' not in chain:\n",
    "                    for order_head in order_head_lst:\n",
    "                        if chain[-1] == order_head[1]:\n",
    "                            heads.append(order_head[0])\n",
    "                    if heads == [] and 'stop' not in chain:\n",
    "                        chain.append('stop')\n",
    "                    else:\n",
    "                        ind_head = 0\n",
    "                        for head in heads:\n",
    "                            new_chain = copy.copy(chain)[:-1]\n",
    "                            if ind_head == 0:\n",
    "                                chain.append(head)\n",
    "                                ind_head += 1\n",
    "                            else:\n",
    "                                new_chain.append(head)\n",
    "                                chains.append(new_chain)\n",
    "            i += 1\n",
    "        while all(item[-1] == 'stop' for item in chains) is False:\n",
    "            self.chains_heads(chains, order_head_lst)\n",
    "        return chains\n",
    "\n",
    "    def count_depth_for_one_sent(self, sent):\n",
    "        chains, order_head_lst = self.root_children(sent)\n",
    "        chains = self.chains_heads(chains, order_head_lst)\n",
    "        depths = []\n",
    "        for chain in chains:\n",
    "            depths.append(len(chain)-2)\n",
    "        return max(depths)\n",
    "\n",
    "    def count_depths(self):\n",
    "        max_depths = []\n",
    "        sentances, df_sentences = parser.conllu2df(sentences=True)\n",
    "        for sent in df_sentences:\n",
    "            max_depths.append(self.count_depth_for_one_sent(sent))\n",
    "        return max_depths\n",
    "    \n",
    "    def av_depth(self):\n",
    "        max_depths = self.count_depths()\n",
    "        return round(np.mean(max_depths), 2)\n",
    "    \n",
    "    def max_depth(self):\n",
    "        max_depths = self.count_depths()\n",
    "        return round(np.max(max_depths),2)\n",
    "    \n",
    "    def min_depth(self):\n",
    "        max_depths = self.count_depths()\n",
    "        return round(np.min(max_depths), 2)\n",
    "    \n",
    "    def find_in_dict(self, d, v):\n",
    "        try:\n",
    "            return d[v]\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def count_dep_sent(self):\n",
    "        dep_rel = self.get_dep_rel()           \n",
    "        dict_dep_rel = collections.Counter(dep_rel)\n",
    "        acl = self.find_in_dict(dict_dep_rel, 'acl')\n",
    "        rel_cl = self.find_in_dict(dict_dep_rel, 'acl:relcl')\n",
    "        advcl = self.find_in_dict(dict_dep_rel, 'advcl')\n",
    "        return acl, rel_cl, advcl\n",
    "    \n",
    "    def count_sent(parsed_text):\n",
    "        sentances, df_sentences = parser.conllu2df(sentences=True)\n",
    "        return len(sentances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parsing import ParserUDpipe\n",
    "with open('/Users/irene/Desktop/Диплом/new_data/1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "text\n",
    "parser = ParserUDpipe(text)\n",
    "SC = SyntacticComplexity(text)\n",
    "SC.count_dep_sent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhetorical Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('5000frequentCOCA.csv', 'r') as file:\n",
    "#    f = file.read()\n",
    "#f = f.replace('  ', '')\n",
    "#with open('5000frequentCOCA.csv', 'w') as file:\n",
    "#    file.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('5000frequentCOCA.csv')\n",
    "df2 = pd.read_csv('frequentverbsCOCAfrom5000.csv')\n",
    "with open('UWL.txt', 'r') as file:\n",
    "    f = file.read()\n",
    "uwl = f.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import io\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str\n",
    "\n",
    "# Write JSON file\n",
    "data = {'5000frequentCOCA': list(df1['Word']), \n",
    "        'frequentverbsCOCAfrom5000': list(df1['Word']), \n",
    "        'UWL': uwl}\n",
    "with io.open('lists.json', 'w', encoding='utf8') as outfile:\n",
    "    str_ = json.dumps(data,\n",
    "                      indent=4, sort_keys=True,\n",
    "                      separators=(',', ': '), ensure_ascii=False)\n",
    "    outfile.write(to_unicode(str_))\n",
    "\n",
    "# Read JSON file\n",
    "with open('lists.json') as data_file:\n",
    "    data_loaded = json.load(data_file)\n",
    "\n",
    "print(data == data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверить ord/lemma/token/verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('5000frequentCOCA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Word</th>\n",
       "      <th>Part_of_speech</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Dispersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>22038615</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>be</td>\n",
       "      <td>v</td>\n",
       "      <td>12545825</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>c</td>\n",
       "      <td>10741073</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>of</td>\n",
       "      <td>i</td>\n",
       "      <td>10343885</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>10144200</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank Word Part_of_speech  Frequency  Dispersion\n",
       "0     1  the              a   22038615        0.98\n",
       "1     2   be              v   12545825        0.97\n",
       "2     3  and              c   10741073        0.99\n",
       "3     4   of              i   10343885        0.97\n",
       "4     5    a              a   10144200        0.98"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
