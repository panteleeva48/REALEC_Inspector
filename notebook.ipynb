{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import ParserUDpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = ParserUDpipe(\"It is an example.\")\n",
    "#df = parser.conllu2df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dolphins love little dolphins.\n",
    "forms = 3: 'love', 'dolphins', 'little'\n",
    "lemmas = 3: 'love', 'dolphin', 'little'\n",
    "tokens = 4: 'dolphins', 'love', 'dolphins', 'little'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверить на punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "\n",
    "open_class = [\"NOUN\", \"VERB\", \"ADV\", \"ADJ\"]\n",
    "with open('lists.json') as data_file:\n",
    "    lists = json.load(data_file)\n",
    "fivetfrequentCOCA = lists['5000frequentCOCA']\n",
    "frequentverbsCOCAfromfivet = lists['frequentverbsCOCAfrom5000']\n",
    "uwl = lists['UWL']\n",
    "\n",
    "class LexicalComplexity:\n",
    "    \"\"\"Returns values of lexical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    def get_verb_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'VERB']['Lemma']\n",
    "    \n",
    "    def get_noun_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'NOUN']['Lemma']\n",
    "\n",
    "    def get_adj_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADJ']['Lemma']\n",
    "    \n",
    "    def get_adv_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADV']['Lemma']\n",
    "    \n",
    "    def get_lex_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'].isin(open_class)]['Lemma']\n",
    "    \n",
    "    def get_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Lemma']\n",
    "    \n",
    "    def devision(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def corrected_devision(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(2*len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def root_devision(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def squared_devision(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)**2/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def log_devision(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))/math.log(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def uber(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))**2/math.log(len(set(list2))/len(list1))\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def density(self, punct=False):\n",
    "        \"\"\"\n",
    "        number of lexical tokens/number of tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        lemmas = self.get_lemmas()\n",
    "        return self.devision(lex_lemmas, lemmas)\n",
    "    \n",
    "    def LS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated lexical tokens/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        soph_lex_lemmas = [i for i in lex_lemmas if i not in fivetfrequentCOCA]\n",
    "        return self.devision(soph_lex_lemmas, lex_lemmas)\n",
    "    \n",
    "    def VS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated verb lemmas/number of verb tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = self.get_verb_lemmas()\n",
    "        soph_verbs = set([i for i in verb_lemmas if i not in frequentverbsCOCAfromfivet])\n",
    "        VSI = self.devision(soph_verbs, verb_lemmas)\n",
    "        VSII = self.corrected_devision(soph_verbs, verb_lemmas)\n",
    "        VSIII = self.squared_devision(soph_verbs, verb_lemmas)\n",
    "        return VSI, VSII, VSIII\n",
    "\n",
    "    def LFP(self):\n",
    "        \"\"\"\n",
    "        Lexical Frequency Profile is the proportion of tokens:\n",
    "        first - 1000 most frequent words\n",
    "        second list - the second 1000\n",
    "        third - University Word List (Xue & Nation 1989)\n",
    "        none - list of those that are not in these lists\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        first = [i for i in lemmas if i in fivetfrequentCOCA[0:1000]]\n",
    "        second = [i for i in lemmas if i in fivetfrequentCOCA[1000:2000]]\n",
    "        third = [i for i in lemmas if i in uwl]\n",
    "        first_procent = self.devision(first, lemmas)\n",
    "        second_procent = self.devision(second, lemmas)\n",
    "        third_procent = self.devision(third, lemmas)\n",
    "        none = 1 - (first_procent + second_procent + third_procent)\n",
    "        return first_procent, second_procent , third_procent, none\n",
    "    \n",
    "    def NDW(self):\n",
    "        \"\"\"\n",
    "        number of lemmas\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        return len(set(lemmas))\n",
    "    \n",
    "    def TTR(self):\n",
    "        \"\"\"\n",
    "        number of lemmas/number of tokens\n",
    "        \"\"\"\n",
    "        lemmas = set(self.get_lemmas())\n",
    "        tokens = self.get_lemmas()\n",
    "        TTR = self.devision(lemmas, tokens)\n",
    "        CTTR = self.corrected_devision(lemmas, tokens)\n",
    "        RTTR = self.root_devision(lemmas, tokens)\n",
    "        LogTTR = self.log_devision(lemmas, tokens)\n",
    "        Uber = self.uber(lemmas, tokens)\n",
    "        D = None\n",
    "        return TTR, CTTR, RTTR, LogTTR, Uber, D\n",
    "\n",
    "    def LV(self):\n",
    "        \"\"\"\n",
    "        number of lexical lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = set(self.get_lex_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return len(lex_lemmas)/len(lex_tokens)\n",
    "    \n",
    "    def VV(self):\n",
    "        \"\"\"\n",
    "        VVI: number of verb lemmas/number of verb tokens\n",
    "        VVII: number of verb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = set(self.get_verb_lemmas())\n",
    "        verb_tokens = self.get_verb_lemmas()\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        VVI = self.devision(verb_lemmas, verb_tokens)\n",
    "        SVVI = self.squared_devision(verb_lemmas, verb_tokens)\n",
    "        CVVI = self.corrected_devision(verb_lemmas, verb_tokens)\n",
    "        VVII = self.devision(verb_lemmas, lex_tokens)\n",
    "        return VVI, SVVI, CVVI, VVII\n",
    "        \n",
    "    def NV(self):\n",
    "        \"\"\"\n",
    "        number of noun lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        noun_lemmas = set(self.get_noun_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.devision(noun_lemmas, lex_tokens)\n",
    "\n",
    "    def AdjV(self):\n",
    "        \"\"\"\n",
    "        number of adjective lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adj_lemmas = set(self.get_adj_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.devision(adj_lemmas, lex_tokens)\n",
    "    \n",
    "    def AdvV(self):\n",
    "        \"\"\"\n",
    "        number of adverb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adv_lemmas = set(self.get_adv_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.devision(adv_lemmas, lex_tokens)\n",
    "    \n",
    "    def ModV(self):\n",
    "        return self.AdjV() + self.AdvV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"He hisses him.\"\n",
    "parser = ParserUDpipe(example)\n",
    "#print(parser.conllu2df())\n",
    "LC = LexicalComplexity(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lex_comp = {'density': LC.density(), 'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "                 'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "                 'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "                 'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "                 'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.TTR()[5], \n",
    "                 'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "                 'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "                 'ModV': LC.ModV()}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AdjV': 0.0,\n",
       " 'AdvV': 0.0,\n",
       " 'CTTR': 1.0606601717798212,\n",
       " 'CVVI': 0.7071067811865475,\n",
       " 'D': None,\n",
       " 'LFP_first': 0.5,\n",
       " 'LFP_none': 0.5,\n",
       " 'LFP_second': 0.0,\n",
       " 'LFP_third': 0.0,\n",
       " 'LS': 1.0,\n",
       " 'LV': 1.0,\n",
       " 'LogTTR': 0.7924812503605781,\n",
       " 'ModV': 0.0,\n",
       " 'NDW': 3,\n",
       " 'NV': 0.0,\n",
       " 'RTTR': 1.5,\n",
       " 'SVVI': 1.0,\n",
       " 'TTR': 0.75,\n",
       " 'Uber': 0,\n",
       " 'VSI': 1.0,\n",
       " 'VSII': 0.7071067811865475,\n",
       " 'VSIII': 1.0,\n",
       " 'VVI': 1.0,\n",
       " 'VVII': 1.0,\n",
       " 'density': 0.25}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_lex_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('5000frequentCOCA.csv', 'r') as file:\n",
    "#    f = file.read()\n",
    "#f = f.replace('  ', '')\n",
    "#with open('5000frequentCOCA.csv', 'w') as file:\n",
    "#    file.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('5000frequentCOCA.csv')\n",
    "df2 = pd.read_csv('frequentverbsCOCAfrom5000.csv')\n",
    "with open('UWL.txt', 'r') as file:\n",
    "    f = file.read()\n",
    "uwl = f.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import io\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str\n",
    "\n",
    "# Write JSON file\n",
    "data = {'5000frequentCOCA': list(df1['Word']), \n",
    "        'frequentverbsCOCAfrom5000': list(df1['Word']), \n",
    "        'UWL': uwl}\n",
    "with io.open('lists.json', 'w', encoding='utf8') as outfile:\n",
    "    str_ = json.dumps(data,\n",
    "                      indent=4, sort_keys=True,\n",
    "                      separators=(',', ': '), ensure_ascii=False)\n",
    "    outfile.write(to_unicode(str_))\n",
    "\n",
    "# Read JSON file\n",
    "with open('lists.json') as data_file:\n",
    "    data_loaded = json.load(data_file)\n",
    "\n",
    "print(data == data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверить ord/lemma/token/verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('5000frequentCOCA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Word</th>\n",
       "      <th>Part_of_speech</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Dispersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>22038615</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>be</td>\n",
       "      <td>v</td>\n",
       "      <td>12545825</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>c</td>\n",
       "      <td>10741073</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>of</td>\n",
       "      <td>i</td>\n",
       "      <td>10343885</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>10144200</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank Word Part_of_speech  Frequency  Dispersion\n",
       "0     1  the              a   22038615        0.98\n",
       "1     2   be              v   12545825        0.97\n",
       "2     3  and              c   10741073        0.99\n",
       "3     4   of              i   10343885        0.97\n",
       "4     5    a              a   10144200        0.98"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
