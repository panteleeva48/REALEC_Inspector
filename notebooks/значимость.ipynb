{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/irene/Desktop/Диплом/code/result_criteria/result_criteria.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Essay</th>\n",
       "      <th>Class</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aver</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aver</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>worst</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>worst</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Essay  Class  Type\n",
       "0      1   aver     1\n",
       "1      2   aver     1\n",
       "2      3   aver     1\n",
       "3      4  worst     2\n",
       "4      5  worst     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лексические"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>density</th>\n",
       "      <th>LS</th>\n",
       "      <th>VSI</th>\n",
       "      <th>VSII</th>\n",
       "      <th>VSIII</th>\n",
       "      <th>LFP_first</th>\n",
       "      <th>LFP_second</th>\n",
       "      <th>LFP_third</th>\n",
       "      <th>LFP_none</th>\n",
       "      <th>...</th>\n",
       "      <th>VVI</th>\n",
       "      <th>SVVI</th>\n",
       "      <th>CVVI</th>\n",
       "      <th>VVII</th>\n",
       "      <th>NV</th>\n",
       "      <th>AdjV</th>\n",
       "      <th>AdvV</th>\n",
       "      <th>ModV</th>\n",
       "      <th>Type</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>0.40136</td>\n",
       "      <td>0.25424</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63265</td>\n",
       "      <td>0.08844</td>\n",
       "      <td>0.04082</td>\n",
       "      <td>0.23810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>6.40000</td>\n",
       "      <td>1.78885</td>\n",
       "      <td>0.13559</td>\n",
       "      <td>0.38983</td>\n",
       "      <td>0.06780</td>\n",
       "      <td>0.06780</td>\n",
       "      <td>0.13559</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>0.44541</td>\n",
       "      <td>0.19608</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.63319</td>\n",
       "      <td>0.12227</td>\n",
       "      <td>0.04367</td>\n",
       "      <td>0.20087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45000</td>\n",
       "      <td>4.05000</td>\n",
       "      <td>1.42302</td>\n",
       "      <td>0.08824</td>\n",
       "      <td>0.26471</td>\n",
       "      <td>0.11765</td>\n",
       "      <td>0.12745</td>\n",
       "      <td>0.24510</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>0.44762</td>\n",
       "      <td>0.16489</td>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.24254</td>\n",
       "      <td>0.11765</td>\n",
       "      <td>0.69762</td>\n",
       "      <td>0.09286</td>\n",
       "      <td>0.03571</td>\n",
       "      <td>0.17381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76471</td>\n",
       "      <td>19.88235</td>\n",
       "      <td>3.15296</td>\n",
       "      <td>0.13830</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.20213</td>\n",
       "      <td>0.12234</td>\n",
       "      <td>0.32447</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>0.43155</td>\n",
       "      <td>0.17931</td>\n",
       "      <td>0.08824</td>\n",
       "      <td>0.36380</td>\n",
       "      <td>0.26471</td>\n",
       "      <td>0.72024</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.03571</td>\n",
       "      <td>0.17560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88235</td>\n",
       "      <td>26.47059</td>\n",
       "      <td>3.63803</td>\n",
       "      <td>0.20690</td>\n",
       "      <td>0.35172</td>\n",
       "      <td>0.10345</td>\n",
       "      <td>0.12414</td>\n",
       "      <td>0.22759</td>\n",
       "      <td>2</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.txt</td>\n",
       "      <td>0.40306</td>\n",
       "      <td>0.20253</td>\n",
       "      <td>0.21429</td>\n",
       "      <td>0.56695</td>\n",
       "      <td>0.64286</td>\n",
       "      <td>0.64796</td>\n",
       "      <td>0.11735</td>\n",
       "      <td>0.07143</td>\n",
       "      <td>0.16327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>10.28571</td>\n",
       "      <td>2.26779</td>\n",
       "      <td>0.15190</td>\n",
       "      <td>0.32911</td>\n",
       "      <td>0.11392</td>\n",
       "      <td>0.07595</td>\n",
       "      <td>0.18987</td>\n",
       "      <td>1</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay  density       LS      VSI     VSII    VSIII  LFP_first  LFP_second  \\\n",
       "0  1.txt  0.40136  0.25424  0.00000  0.00000  0.00000    0.63265     0.08844   \n",
       "1  2.txt  0.44541  0.19608  0.00000  0.00000  0.00000    0.63319     0.12227   \n",
       "2  3.txt  0.44762  0.16489  0.05882  0.24254  0.11765    0.69762     0.09286   \n",
       "3  4.txt  0.43155  0.17931  0.08824  0.36380  0.26471    0.72024     0.06845   \n",
       "4  5.txt  0.40306  0.20253  0.21429  0.56695  0.64286    0.64796     0.11735   \n",
       "\n",
       "   LFP_third  LFP_none  ...        VVI      SVVI     CVVI     VVII       NV  \\\n",
       "0    0.04082   0.23810  ...    0.80000   6.40000  1.78885  0.13559  0.38983   \n",
       "1    0.04367   0.20087  ...    0.45000   4.05000  1.42302  0.08824  0.26471   \n",
       "2    0.03571   0.17381  ...    0.76471  19.88235  3.15296  0.13830  0.25000   \n",
       "3    0.03571   0.17560  ...    0.88235  26.47059  3.63803  0.20690  0.35172   \n",
       "4    0.07143   0.16327  ...    0.85714  10.28571  2.26779  0.15190  0.32911   \n",
       "\n",
       "      AdjV     AdvV     ModV  Type  Class  \n",
       "0  0.06780  0.06780  0.13559     1   aver  \n",
       "1  0.11765  0.12745  0.24510     1   aver  \n",
       "2  0.20213  0.12234  0.32447     1   aver  \n",
       "3  0.10345  0.12414  0.22759     2  worst  \n",
       "4  0.11392  0.07595  0.18987     1  worst  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_lex = ['essay', 'density', 'LS', 'VSI', 'VSII', 'VSIII', 'LFP_first',\n",
    "            'LFP_second', 'LFP_third', 'LFP_none', 'NDW', 'TTR', \n",
    "            'CTTR', 'RTTR', 'LogTTR', 'Uber', 'D', 'LV', 'VVI',\n",
    "            'SVVI', 'CVVI', 'VVII', 'NV', 'AdjV', 'AdvV', 'ModV', 'index']\n",
    "lex_criteria = pd.read_csv('/Users/irene/Desktop/Диплом/code/result_criteria/lex_criteria.csv',\n",
    "                           delimiter=',', names=head_lex, index_col=None)\n",
    "lex_criteria = lex_criteria.drop(['index'], axis=1)\n",
    "lex_criteria['Type' ] = df['Type']\n",
    "lex_criteria['Class'] = df['Class']\n",
    "lex_criteria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = lex_criteria[lex_criteria['Class'] == 'best']\n",
    "worst = lex_criteria[lex_criteria['Class'] == 'worst']\n",
    "aver = lex_criteria[lex_criteria['Class'] == 'aver']\n",
    "best_1 = lex_criteria[(lex_criteria['Class'] == 'best') & (df['Type'] == 1)]\n",
    "worst_1 = lex_criteria[(lex_criteria['Class'] == 'worst') & (df['Type'] == 1)]\n",
    "aver_1 = lex_criteria[(lex_criteria['Class'] == 'aver') & (df['Type'] == 1)]\n",
    "best_2 = lex_criteria[(lex_criteria['Class'] == 'best') & (df['Type'] == 2)]\n",
    "worst_2 = lex_criteria[(lex_criteria['Class'] == 'worst') & (df['Type'] == 2)]\n",
    "aver_2 = lex_criteria[(lex_criteria['Class'] == 'aver') & (df['Type'] == 2)]\n",
    "first = lex_criteria[lex_criteria['Type'] == 1]\n",
    "second = lex_criteria[lex_criteria['Type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 74, 157)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best), len(worst), len(aver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 79, 42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_1), len(aver_1), len(worst_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 78, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_2), len(aver_2), len(worst_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 129)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first), len(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criteria = ['density', 'LS', 'VSI', 'VSII', 'VSIII', 'LFP_first',\n",
    "            'LFP_second', 'LFP_third', 'LFP_none', 'NDW', 'TTR', \n",
    "            'CTTR', 'RTTR', 'LogTTR', 'Uber', 'D', 'LV', 'VVI',\n",
    "            'SVVI', 'CVVI', 'VVII', 'NV', 'AdjV', 'AdvV', 'ModV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Критерий: density\n",
      "t = 2.03533\n",
      "p = 0.08898\n",
      "Критерий: LS\n",
      "t = 0.1005\n",
      "p = 1.8403\n",
      "Критерий: VSI\n",
      "t = 2.29084\n",
      "p = 0.04819\n",
      "Статистически значимая разница по критерию 'VSI'\n",
      "Критерий: VSII\n",
      "t = 4.13251\n",
      "p = 0.00015\n",
      "Статистически значимая разница по критерию 'VSII'\n",
      "Критерий: VSIII\n",
      "t = 2.28182\n",
      "p = 0.04928\n",
      "Статистически значимая разница по критерию 'VSIII'\n",
      "Критерий: LFP_first\n",
      "t = 1.83522\n",
      "p = 0.13895\n",
      "Критерий: LFP_second\n",
      "t = 3.36182\n",
      "p = 0.0022\n",
      "Статистически значимая разница по критерию 'LFP_second'\n",
      "Критерий: LFP_third\n",
      "t = 1.69028\n",
      "p = 0.18824\n",
      "Критерий: LFP_none\n",
      "t = -3.27162\n",
      "p = 0.00294\n",
      "Статистически значимая разница по критерию 'LFP_none'\n",
      "Критерий: NDW\n",
      "t = 5.57271\n",
      "p = 0.0\n",
      "Статистически значимая разница по критерию 'NDW'\n",
      "Критерий: TTR\n",
      "t = -0.96653\n",
      "p = 0.67227\n",
      "Критерий: CTTR\n",
      "t = 4.9867\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'CTTR'\n",
      "Критерий: RTTR\n",
      "t = 4.9867\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'RTTR'\n",
      "Критерий: LogTTR\n",
      "t = 0.83088\n",
      "p = 0.81608\n",
      "Критерий: Uber\n",
      "t = nan\n",
      "p = nan\n",
      "Критерий: D\n",
      "t = 2.04536\n",
      "p = 0.08694\n",
      "Критерий: LV\n",
      "t = 1.24448\n",
      "p = 0.43252\n",
      "Критерий: VVI\n",
      "t = 0.54361\n",
      "p = 1.17586\n",
      "Критерий: SVVI\n",
      "t = 6.27493\n",
      "p = 0.0\n",
      "Статистически значимая разница по критерию 'SVVI'\n",
      "Критерий: CVVI\n",
      "t = 5.78637\n",
      "p = 0.0\n",
      "Статистически значимая разница по критерию 'CVVI'\n",
      "Критерий: VVII\n",
      "t = 2.92242\n",
      "p = 0.00861\n",
      "Статистически значимая разница по критерию 'VVII'\n",
      "Критерий: NV\n",
      "t = 0.67328\n",
      "p = 1.00468\n",
      "Критерий: AdjV\n",
      "t = -1.7401\n",
      "p = 0.1699\n",
      "Критерий: AdvV\n",
      "t = 0.09802\n",
      "p = 1.84423\n",
      "Критерий: ModV\n",
      "t = -1.34282\n",
      "p = 0.3648\n"
     ]
    }
   ],
   "source": [
    "for c in criteria:\n",
    "    t, p = stats.ttest_ind(best[c],worst[c])\n",
    "    print('Критерий:', c)\n",
    "    print(\"t = \" + str(round(t, 5)))\n",
    "    print(\"p = \" + str(round(2*p, 5)))\n",
    "    if 2*p < 0.05:\n",
    "        print(\"Статистически значимая разница по критерию '%s'\" % c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологические"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>der_suff_level3</th>\n",
       "      <th>der_suff_level4</th>\n",
       "      <th>der_suff_level5</th>\n",
       "      <th>der_suff_level6</th>\n",
       "      <th>MCI</th>\n",
       "      <th>freq_finite_forms</th>\n",
       "      <th>freq_aux</th>\n",
       "      <th>num_inf</th>\n",
       "      <th>num_gerund</th>\n",
       "      <th>num_pres_sg</th>\n",
       "      <th>num_pres_pl</th>\n",
       "      <th>num_part</th>\n",
       "      <th>num_past</th>\n",
       "      <th>Type</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.07692</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>0.02326</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02326</td>\n",
       "      <td>0.06977</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>0.16667</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>0.04124</td>\n",
       "      <td>0.17526</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.05155</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.61667</td>\n",
       "      <td>0.43333</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>0.02703</td>\n",
       "      <td>0.12162</td>\n",
       "      <td>0.05405</td>\n",
       "      <td>0.02703</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.51852</td>\n",
       "      <td>0.37037</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.txt</td>\n",
       "      <td>0.04348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04348</td>\n",
       "      <td>0.06522</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.72727</td>\n",
       "      <td>0.36364</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay  der_suff_level3  der_suff_level4  der_suff_level5  der_suff_level6  \\\n",
       "0  1.txt          0.03846          0.03846          0.07692          0.00000   \n",
       "1  2.txt          0.02326          0.00000          0.02326          0.06977   \n",
       "2  3.txt          0.04124          0.17526          0.11340          0.05155   \n",
       "3  4.txt          0.02703          0.12162          0.05405          0.02703   \n",
       "4  5.txt          0.04348          0.00000          0.04348          0.06522   \n",
       "\n",
       "    MCI  freq_finite_forms  freq_aux  num_inf  num_gerund  num_pres_sg  \\\n",
       "0  2.75            0.80000   0.33333        0           0            8   \n",
       "1  3.25            0.66667   0.16667        5           1            5   \n",
       "2  5.25            0.61667   0.43333       15           2           26   \n",
       "3  2.75            0.51852   0.37037       18           3           11   \n",
       "4  2.75            0.72727   0.36364        1           4           10   \n",
       "\n",
       "   num_pres_pl  num_part  num_past  Type  Class  \n",
       "0            2         3         1     1   aver  \n",
       "1           10         1         0     1   aver  \n",
       "2            6         5         2     1   aver  \n",
       "3            8         5         1     2  worst  \n",
       "4            5         1         0     1  worst  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_morph = ['essay', 'der_suff_level3', 'der_suff_level4', 'der_suff_level5', \n",
    "              'der_suff_level6', 'MCI', 'freq_finite_forms', 'freq_aux',\n",
    "              'num_inf', 'num_gerund', 'num_pres_sg', 'num_pres_pl','num_part','num_past', 'index']\n",
    "morph_criteria = pd.read_csv('/Users/irene/Desktop/Диплом/code/result_criteria/morph_criteria.csv',\n",
    "                           delimiter=',', names=head_morph, index_col=None)\n",
    "morph_criteria = morph_criteria.drop(['index'], axis=1)\n",
    "morph_criteria['Type' ] = df['Type']\n",
    "morph_criteria['Class'] = df['Class']\n",
    "morph_criteria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = morph_criteria[morph_criteria['Class'] == 'best']\n",
    "worst = morph_criteria[morph_criteria['Class'] == 'worst']\n",
    "aver = morph_criteria[morph_criteria['Class'] == 'aver']\n",
    "best_1 = morph_criteria[(morph_criteria['Class'] == 'best') & (df['Type'] == 1)]\n",
    "worst_1 = morph_criteria[(morph_criteria['Class'] == 'worst') & (df['Type'] == 1)]\n",
    "aver_1 = morph_criteria[(morph_criteria['Class'] == 'aver') & (df['Type'] == 1)]\n",
    "best_2 = morph_criteria[(morph_criteria['Class'] == 'best') & (df['Type'] == 2)]\n",
    "worst_2 = morph_criteria[(morph_criteria['Class'] == 'worst') & (df['Type'] == 2)]\n",
    "aver_2 = morph_criteria[(morph_criteria['Class'] == 'aver') & (df['Type'] == 2)]\n",
    "first = morph_criteria[morph_criteria['Type'] == 1]\n",
    "second = morph_criteria[morph_criteria['Type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criteria = ['der_suff_level3', 'der_suff_level4', 'der_suff_level5', \n",
    "            'der_suff_level6', 'MCI', 'freq_finite_forms', 'freq_aux',\n",
    "            'num_inf', 'num_gerund', 'num_pres_sg', 'num_pres_pl','num_part','num_past']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Критерий: der_suff_level3\n",
      "t = 0.25722\n",
      "p = 1.59508\n",
      "Критерий: der_suff_level4\n",
      "t = 0.46888\n",
      "p = 1.28038\n",
      "Критерий: der_suff_level5\n",
      "t = 1.83944\n",
      "p = 0.13769\n",
      "Критерий: der_suff_level6\n",
      "t = -0.80542\n",
      "p = 0.84501\n",
      "Критерий: MCI\n",
      "t = 2.3782\n",
      "p = 0.03863\n",
      "Статистически значимая разница по критерию 'MCI'\n",
      "Критерий: freq_finite_forms\n",
      "t = -3.77053\n",
      "p = 0.00055\n",
      "Статистически значимая разница по критерию 'freq_finite_forms'\n",
      "Критерий: freq_aux\n",
      "t = -1.1593\n",
      "p = 0.49824\n",
      "Критерий: num_inf\n",
      "t = 3.66256\n",
      "p = 0.00081\n",
      "Статистически значимая разница по критерию 'num_inf'\n",
      "Критерий: num_gerund\n",
      "t = 4.76415\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'num_gerund'\n",
      "Критерий: num_pres_sg\n",
      "t = 2.58924\n",
      "p = 0.02214\n",
      "Статистически значимая разница по критерию 'num_pres_sg'\n",
      "Критерий: num_pres_pl\n",
      "t = 2.29861\n",
      "p = 0.04726\n",
      "Статистически значимая разница по критерию 'num_pres_pl'\n",
      "Критерий: num_part\n",
      "t = 3.03413\n",
      "p = 0.00616\n",
      "Статистически значимая разница по критерию 'num_part'\n",
      "Критерий: num_past\n",
      "t = -0.49927\n",
      "p = 1.23739\n"
     ]
    }
   ],
   "source": [
    "for c in criteria:\n",
    "    t, p = stats.ttest_ind(best[c],worst[c])\n",
    "    print('Критерий:', c)\n",
    "    print(\"t = \" + str(round(t, 5)))\n",
    "    print(\"p = \" + str(round(2*p, 5)))\n",
    "    if 2*p < 0.05:\n",
    "        print(\"Статистически значимая разница по критерию '%s'\" % c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Синтаксические"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>av_depth</th>\n",
       "      <th>mean_l_sim</th>\n",
       "      <th>mean_p_sim</th>\n",
       "      <th>mean_l_sim_nei</th>\n",
       "      <th>mean_p_sim_nei</th>\n",
       "      <th>mean_length_s</th>\n",
       "      <th>mean_length_c</th>\n",
       "      <th>c_s</th>\n",
       "      <th>c_t</th>\n",
       "      <th>acl_t</th>\n",
       "      <th>...</th>\n",
       "      <th>ger_inf</th>\n",
       "      <th>part_n</th>\n",
       "      <th>n_inf</th>\n",
       "      <th>num_vp</th>\n",
       "      <th>min_depth</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>num_np</th>\n",
       "      <th>Type</th>\n",
       "      <th>Class</th>\n",
       "      <th>Essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.00</td>\n",
       "      <td>13.64</td>\n",
       "      <td>9.73</td>\n",
       "      <td>13.20</td>\n",
       "      <td>8.80</td>\n",
       "      <td>11.73</td>\n",
       "      <td>9.923</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.58</td>\n",
       "      <td>18.89</td>\n",
       "      <td>14.32</td>\n",
       "      <td>19.36</td>\n",
       "      <td>14.64</td>\n",
       "      <td>17.33</td>\n",
       "      <td>12.235</td>\n",
       "      <td>1.417</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.68</td>\n",
       "      <td>20.84</td>\n",
       "      <td>16.16</td>\n",
       "      <td>19.81</td>\n",
       "      <td>15.10</td>\n",
       "      <td>17.41</td>\n",
       "      <td>10.639</td>\n",
       "      <td>1.636</td>\n",
       "      <td>1.029</td>\n",
       "      <td>0.086</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.40</td>\n",
       "      <td>24.36</td>\n",
       "      <td>18.55</td>\n",
       "      <td>25.07</td>\n",
       "      <td>18.71</td>\n",
       "      <td>20.27</td>\n",
       "      <td>11.259</td>\n",
       "      <td>1.800</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>worst</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.50</td>\n",
       "      <td>40.67</td>\n",
       "      <td>34.67</td>\n",
       "      <td>35.20</td>\n",
       "      <td>29.00</td>\n",
       "      <td>30.50</td>\n",
       "      <td>12.200</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>worst</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   av_depth  mean_l_sim  mean_p_sim  mean_l_sim_nei  mean_p_sim_nei  \\\n",
       "0      4.00       13.64        9.73           13.20            8.80   \n",
       "1      4.58       18.89       14.32           19.36           14.64   \n",
       "2      4.68       20.84       16.16           19.81           15.10   \n",
       "3      4.40       24.36       18.55           25.07           18.71   \n",
       "4      5.50       40.67       34.67           35.20           29.00   \n",
       "\n",
       "   mean_length_s  mean_length_c    c_s    c_t  acl_t  ...    ger_inf  part_n  \\\n",
       "0          11.73          9.923  1.182  1.000  0.077  ...          0       0   \n",
       "1          17.33         12.235  1.417  1.000  0.118  ...          5       1   \n",
       "2          17.41         10.639  1.636  1.029  0.086  ...         15       0   \n",
       "3          20.27         11.259  1.800  1.000  0.185  ...         17       0   \n",
       "4          30.50         12.200  2.500  1.000  0.133  ...          5       0   \n",
       "\n",
       "   n_inf  num_vp  min_depth  max_depth  num_np  Type  Class  Essay  \n",
       "0      1      10          2          5      52     1   aver      1  \n",
       "1      0      18          3          7      87     1   aver      2  \n",
       "2      1      33          2         12     105     1   aver      3  \n",
       "3      9      34          3          6      83     2  worst      4  \n",
       "4      0      15          2         12      72     1  worst      5  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_syn = ['av_depth', 'mean_l_sim', 'mean_p_sim', 'mean_l_sim_nei', 'mean_p_sim_nei',\n",
    "            'mean_length_s', 'mean_length_c', 'c_s', 'c_t',\n",
    "            'acl_t', 'acl_relcl_t', 'advcl_t', 'acl_cl', 'acl_relcl_cl', 'advcl_cl',\n",
    "            'coord_cl', 't_s', 'poss_s', 'prep_s', 'adj_n_s', 'ger_inf_s', 'part_n_s', 'n_inf_s',\n",
    "            'vp_s', 'mean_tokens_root',\n",
    "            'num_tokens', 'num_acl', 'num_acl_relcl', 'num_advcl',\n",
    "            'num_sents', 'num_cl', 'num_tu', 'num_ctu', \n",
    "            'num_coord', 'poss', 'prep_ph', 'adj_n', 'ger_inf',\n",
    "            'part_n', 'n_inf', 'num_vp','min_depth', 'max_depth', 'num_np', 'index']\n",
    "syn_criteria = pd.read_csv('/Users/irene/Desktop/Диплом/code/result_criteria/syn_criteria.csv',\n",
    "                           delimiter=',', names=head_syn, index_col=None)\n",
    "syn_criteria = syn_criteria.drop(['index'], axis=1)\n",
    "syn_criteria['Type' ] = df['Type']\n",
    "syn_criteria['Class'] = df['Class']\n",
    "syn_criteria['Essay'] = df['Essay']\n",
    "syn_criteria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = syn_criteria[syn_criteria['Class'] == 'best']\n",
    "worst = syn_criteria[syn_criteria['Class'] == 'worst']\n",
    "aver = syn_criteria[syn_criteria['Class'] == 'aver']\n",
    "best_1 = syn_criteria[(syn_criteria['Class'] == 'best') & (df['Type'] == 1)]\n",
    "worst_1 = syn_criteria[(syn_criteria['Class'] == 'worst') & (df['Type'] == 1)]\n",
    "aver_1 = syn_criteria[(syn_criteria['Class'] == 'aver') & (df['Type'] == 1)]\n",
    "best_2 = syn_criteria[(syn_criteria['Class'] == 'best') & (df['Type'] == 2)]\n",
    "worst_2 = syn_criteria[(syn_criteria['Class'] == 'worst') & (df['Type'] == 2)]\n",
    "aver_2 = syn_criteria[(syn_criteria['Class'] == 'aver') & (df['Type'] == 2)]\n",
    "first = syn_criteria[syn_criteria['Type'] == 1]\n",
    "second = syn_criteria[syn_criteria['Type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criteria = ['av_depth', 'mean_l_sim', 'mean_p_sim', 'mean_l_sim_nei', 'mean_p_sim_nei',\n",
    "            'mean_length_s', 'mean_length_c', 'c_s', 'c_t',\n",
    "            'acl_t', 'acl_relcl_t', 'advcl_t', 'acl_cl', 'acl_relcl_cl', 'advcl_cl',\n",
    "            'coord_cl', 't_s', 'poss_s', 'prep_s', 'adj_n_s', 'ger_inf_s', 'part_n_s', 'n_inf_s',\n",
    "            'vp_s', 'mean_tokens_root',\n",
    "            'num_tokens', 'num_acl', 'num_acl_relcl', 'num_advcl',\n",
    "            'num_sents', 'num_cl', 'num_tu', 'num_ctu', \n",
    "            'num_coord', 'poss', 'prep_ph', 'adj_n', 'ger_inf',\n",
    "            'part_n', 'n_inf', 'num_vp','min_depth', 'max_depth', 'num_np']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Критерий: av_depth\n",
      "t = 4.17692\n",
      "p = 0.00013\n",
      "Статистически значимая разница по критерию 'av_depth'\n",
      "Критерий: mean_l_sim\n",
      "t = 4.74822\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'mean_l_sim'\n",
      "Критерий: mean_p_sim\n",
      "t = 4.05805\n",
      "p = 0.0002\n",
      "Статистически значимая разница по критерию 'mean_p_sim'\n",
      "Критерий: mean_l_sim_nei\n",
      "t = 4.7857\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'mean_l_sim_nei'\n",
      "Критерий: mean_p_sim_nei\n",
      "t = 4.00327\n",
      "p = 0.00024\n",
      "Статистически значимая разница по критерию 'mean_p_sim_nei'\n",
      "Критерий: mean_length_s\n",
      "t = 4.46164\n",
      "p = 4e-05\n",
      "Статистически значимая разница по критерию 'mean_length_s'\n",
      "Критерий: mean_length_c\n",
      "t = 0.3434\n",
      "p = 1.46405\n",
      "Критерий: c_s\n",
      "t = 3.83076\n",
      "p = 0.00045\n",
      "Статистически значимая разница по критерию 'c_s'\n",
      "Критерий: c_t\n",
      "t = 0.56767\n",
      "p = 1.14309\n",
      "Критерий: acl_t\n",
      "t = 2.33144\n",
      "p = 0.04351\n",
      "Статистически значимая разница по критерию 'acl_t'\n",
      "Критерий: acl_relcl_t\n",
      "t = 2.82871\n",
      "p = 0.01132\n",
      "Статистически значимая разница по критерию 'acl_relcl_t'\n",
      "Критерий: advcl_t\n",
      "t = 3.16318\n",
      "p = 0.00415\n",
      "Статистически значимая разница по критерию 'advcl_t'\n",
      "Критерий: acl_cl\n",
      "t = 2.25071\n",
      "p = 0.05323\n",
      "Критерий: acl_relcl_cl\n",
      "t = 2.81537\n",
      "p = 0.01176\n",
      "Статистически значимая разница по критерию 'acl_relcl_cl'\n",
      "Критерий: advcl_cl\n",
      "t = 3.21934\n",
      "p = 0.00348\n",
      "Статистически значимая разница по критерию 'advcl_cl'\n",
      "Критерий: coord_cl\n",
      "t = 0.23311\n",
      "p = 1.63231\n",
      "Критерий: t_s\n",
      "t = 3.78098\n",
      "p = 0.00053\n",
      "Статистически значимая разница по критерию 't_s'\n",
      "Критерий: poss_s\n",
      "t = 2.18441\n",
      "p = 0.06258\n",
      "Критерий: prep_s\n",
      "t = 1.86198\n",
      "p = 0.13115\n",
      "Критерий: adj_n_s\n",
      "t = 0.65423\n",
      "p = 1.02896\n",
      "Критерий: ger_inf_s\n",
      "t = 4.37086\n",
      "p = 6e-05\n",
      "Статистически значимая разница по критерию 'ger_inf_s'\n",
      "Критерий: part_n_s\n",
      "t = 2.22493\n",
      "p = 0.05671\n",
      "Критерий: n_inf_s\n",
      "t = 4.9017\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'n_inf_s'\n",
      "Критерий: vp_s\n",
      "t = 5.37684\n",
      "p = 0.0\n",
      "Статистически значимая разница по критерию 'vp_s'\n",
      "Критерий: mean_tokens_root\n",
      "t = 0.67865\n",
      "p = 0.99789\n",
      "Критерий: num_tokens\n",
      "t = 5.19561\n",
      "p = 0.0\n",
      "Статистически значимая разница по критерию 'num_tokens'\n",
      "Критерий: num_acl\n",
      "t = 4.62654\n",
      "p = 2e-05\n",
      "Статистически значимая разница по критерию 'num_acl'\n",
      "Критерий: num_acl_relcl\n",
      "t = 4.24476\n",
      "p = 0.0001\n",
      "Статистически значимая разница по критерию 'num_acl_relcl'\n",
      "Критерий: num_advcl\n",
      "t = 4.72794\n",
      "p = 2e-05\n",
      "Статистически значимая разница по критерию 'num_advcl'\n",
      "Критерий: num_sents\n",
      "t = 1.5587\n",
      "p = 0.24451\n",
      "Критерий: num_cl\n",
      "t = 3.41434\n",
      "p = 0.00186\n",
      "Статистически значимая разница по критерию 'num_cl'\n",
      "Критерий: num_tu\n",
      "t = 3.42575\n",
      "p = 0.00179\n",
      "Статистически значимая разница по критерию 'num_tu'\n",
      "Критерий: num_ctu\n",
      "t = 1.0812\n",
      "p = 0.56447\n",
      "Критерий: num_coord\n",
      "t = 2.37805\n",
      "p = 0.03865\n",
      "Статистически значимая разница по критерию 'num_coord'\n",
      "Критерий: poss\n",
      "t = 4.79329\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'poss'\n",
      "Критерий: prep_ph\n",
      "t = 4.69132\n",
      "p = 2e-05\n",
      "Статистически значимая разница по критерию 'prep_ph'\n",
      "Критерий: adj_n\n",
      "t = 1.45019\n",
      "p = 0.30033\n",
      "Критерий: ger_inf\n",
      "t = 4.47496\n",
      "p = 4e-05\n",
      "Статистически значимая разница по критерию 'ger_inf'\n",
      "Критерий: part_n\n",
      "t = 2.40146\n",
      "p = 0.03639\n",
      "Статистически значимая разница по критерию 'part_n'\n",
      "Критерий: n_inf\n",
      "t = 4.55376\n",
      "p = 3e-05\n",
      "Статистически значимая разница по критерию 'n_inf'\n",
      "Критерий: num_vp\n",
      "t = 4.78167\n",
      "p = 1e-05\n",
      "Статистически значимая разница по критерию 'num_vp'\n",
      "Критерий: min_depth\n",
      "t = 0.8144\n",
      "p = 0.83474\n",
      "Критерий: max_depth\n",
      "t = 3.46578\n",
      "p = 0.00157\n",
      "Статистически значимая разница по критерию 'max_depth'\n",
      "Критерий: num_np\n",
      "t = 6.31495\n",
      "p = 0.0\n",
      "Статистически значимая разница по критерию 'num_np'\n"
     ]
    }
   ],
   "source": [
    "for c in criteria:\n",
    "    t, p = stats.ttest_ind(best[c],worst[c])\n",
    "    print('Критерий:', c)\n",
    "    print(\"t = \" + str(round(t, 5)))\n",
    "    print(\"p = \" + str(round(2*p, 5)))\n",
    "    if 2*p < 0.05:\n",
    "        print(\"Статистически значимая разница по критерию '%s'\" % c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Риторические структуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>link_Sentence connectors(Furthermore)</th>\n",
       "      <th>link_Sentence connectors(In addition)</th>\n",
       "      <th>link_Sentence connectors(Moreover)</th>\n",
       "      <th>link_Sentence connectors</th>\n",
       "      <th>link_Phrases linkers(In addition)</th>\n",
       "      <th>link_Phrases linkers</th>\n",
       "      <th>link_Addition</th>\n",
       "      <th>link_subordinators(Although)</th>\n",
       "      <th>link_subordinators(Even though)</th>\n",
       "      <th>...</th>\n",
       "      <th>4grams_Attitudinal/modality(it is not easy)</th>\n",
       "      <th>4grams_Attitudinal/modality(necessary for us to)</th>\n",
       "      <th>4grams_Attitudinal/modality(should learn how to)</th>\n",
       "      <th>4grams_Attitudinal/modality(will not be able to)</th>\n",
       "      <th>4grams_Attitudinal/modality</th>\n",
       "      <th>4grams_Stance</th>\n",
       "      <th>4grams_all</th>\n",
       "      <th>num_4grams</th>\n",
       "      <th>Type</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>aver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay  link_Sentence connectors(Furthermore)  \\\n",
       "0  1.txt                                      0   \n",
       "1  2.txt                                      0   \n",
       "2  3.txt                                      0   \n",
       "3  4.txt                                      1   \n",
       "4  5.txt                                      0   \n",
       "\n",
       "   link_Sentence connectors(In addition)  link_Sentence connectors(Moreover)  \\\n",
       "0                                      0                                   0   \n",
       "1                                      0                                   0   \n",
       "2                                      0                                   0   \n",
       "3                                      0                                   1   \n",
       "4                                      0                                   0   \n",
       "\n",
       "   link_Sentence connectors  link_Phrases linkers(In addition)  \\\n",
       "0                         0                                  0   \n",
       "1                         0                                  0   \n",
       "2                         0                                  0   \n",
       "3                         0                                  0   \n",
       "4                         0                                  0   \n",
       "\n",
       "   link_Phrases linkers  link_Addition  link_subordinators(Although)  \\\n",
       "0                     0              0                             0   \n",
       "1                     0              0                             0   \n",
       "2                     0              0                             1   \n",
       "3                     0              2                             0   \n",
       "4                     0              0                             0   \n",
       "\n",
       "   link_subordinators(Even though)  ...    \\\n",
       "0                                0  ...     \n",
       "1                                0  ...     \n",
       "2                                0  ...     \n",
       "3                                1  ...     \n",
       "4                                0  ...     \n",
       "\n",
       "   4grams_Attitudinal/modality(it is not easy)  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "3                                            0   \n",
       "4                                            0   \n",
       "\n",
       "   4grams_Attitudinal/modality(necessary for us to)  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   4grams_Attitudinal/modality(should learn how to)  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   4grams_Attitudinal/modality(will not be able to)  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   4grams_Attitudinal/modality  4grams_Stance  4grams_all  num_4grams  Type  \\\n",
       "0                            0              0           0           0     1   \n",
       "1                            0              0           0           2     1   \n",
       "2                            0              0           3          15     1   \n",
       "3                            0              0           5          26     2   \n",
       "4                            0              0           0           1     1   \n",
       "\n",
       "   Class  \n",
       "0   aver  \n",
       "1   aver  \n",
       "2   aver  \n",
       "3  worst  \n",
       "4  worst  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_lex = ['essay', 'link_Sentence connectors(Furthermore)',\n",
    " 'link_Sentence connectors(In addition)',\n",
    " 'link_Sentence connectors(Moreover)',\n",
    " 'link_Sentence connectors',\n",
    " 'link_Phrases linkers(In addition)',\n",
    " 'link_Phrases linkers',\n",
    " 'link_Addition',\n",
    " 'link_subordinators(Although)',\n",
    " 'link_subordinators(Even though)',\n",
    " 'link_subordinators',\n",
    " 'link_Sentence connectors(However)',\n",
    " 'link_Sentence connectors(Nevertheless)',\n",
    " 'link_Phrase linkers(despite)',\n",
    " 'link_Phrase linkers(In spite of)',\n",
    " 'link_Phrase linkers',\n",
    " 'link_Adversativity',\n",
    " 'link_subordinators(Because)',\n",
    " 'link_subordinators(since)',\n",
    " 'link_Sentence connectors(Therefore)',\n",
    " 'link_Sentence connectors(As a result)',\n",
    " 'link_Sentence connectors(consequently)',\n",
    " 'link_Sentence connectors(Hence)',\n",
    " 'link_Sentence connectors(Thus)',\n",
    " 'link_Phrase linkers(Because of)',\n",
    " 'link_Phrase linkers(Due to)',\n",
    " 'link_Phrase linkers(As a result of)',\n",
    " 'link_Cause and effect',\n",
    " 'link_Sentence connectors(In other words)',\n",
    " 'link_Sentence connectors(That is)',\n",
    " 'link_Sentence connectors(i.e.)',\n",
    " 'link_Clarification',\n",
    " 'link_Subordinators(while)',\n",
    " 'link_Subordinators(whereas)',\n",
    " 'link_Subordinators',\n",
    " 'link_Sentence connectors(In contrast)',\n",
    " 'link_Sentence connectors(On the other hand)',\n",
    " 'link_Sentence connectors(Conversely)',\n",
    " 'link_Phrase linkers(Unlike)',\n",
    " 'link_Contrast',\n",
    " 'link_Sentence connectors(For example)',\n",
    " 'link_Sentence connectors(For instance)',\n",
    " 'link_Illustration',\n",
    " 'link_Sentence connectors(On the contrary)',\n",
    " 'link_Sentence connectors(As a matter of fact)',\n",
    " 'link_Sentence connectors(In fact)',\n",
    " 'link_Intensification',\n",
    " 'link_all',\n",
    " '4grams_Topic elaboration/clarification(and to be a)',\n",
    " '4grams_Topic elaboration/clarification(are more and more)',\n",
    " '4grams_Topic elaboration/clarification(as well as the)',\n",
    " '4grams_Topic elaboration/clarification(but there are still)',\n",
    " '4grams_Topic elaboration/clarification(can be divided into)',\n",
    " '4grams_Topic elaboration/clarification(how to deal with)',\n",
    " '4grams_Topic elaboration/clarification(if you don’t know)',\n",
    " '4grams_Topic elaboration/clarification(if you want to)',\n",
    " '4grams_Topic elaboration/clarification(in order to make)',\n",
    " '4grams_Topic elaboration/clarification(is a kind of)',\n",
    " '4grams_Topic elaboration/clarification(is based on the)',\n",
    " '4grams_Topic elaboration/clarification(is more important than)',\n",
    " '4grams_Topic elaboration/clarification(is totally different from)',\n",
    " '4grams_Topic elaboration/clarification(it is a good)',\n",
    " '4grams_Topic elaboration/clarification(it is a very)',\n",
    " '4grams_Topic elaboration/clarification(it is also a)',\n",
    " '4grams_Topic elaboration/clarification(it is because the)',\n",
    " '4grams_Topic elaboration/clarification(it is not a)',\n",
    " '4grams_Topic elaboration/clarification(on the other hand)',\n",
    " '4grams_Topic elaboration/clarification(there will be a)',\n",
    " '4grams_Topic elaboration/clarification(to cope with the)',\n",
    " '4grams_Topic elaboration/clarification(want to be a)',\n",
    " '4grams_Topic elaboration/clarification',\n",
    " '4grams_Identification/focus(my point of view)',\n",
    " '4grams_Identification/focus(the best way to)',\n",
    " '4grams_Identification/focus(a very important role)',\n",
    " '4grams_Identification/focus(as far as the)',\n",
    " '4grams_Identification/focus(as I have mentioned)',\n",
    " '4grams_Identification/focus(him or her to)',\n",
    " '4grams_Identification/focus(is one of my)',\n",
    " '4grams_Identification/focus(is one of the)',\n",
    " '4grams_Identification/focus(is the most important)',\n",
    " '4grams_Identification/focus(is very important for)',\n",
    " '4grams_Identification/focus(it is very important)',\n",
    " '4grams_Identification/focus(one of the most)',\n",
    " '4grams_Identification/focus(the most important thing)',\n",
    " '4grams_Identification/focus(we can say that)',\n",
    " '4grams_Identification/focus(we can see that)',\n",
    " '4grams_Identification/focus(we can see the)',\n",
    " '4grams_Identification/focus',\n",
    " '4grams_Topic introduction(I am going to)',\n",
    " '4grams_Topic introduction(I would like to)',\n",
    " '4grams_Topic introduction(if there is a)',\n",
    " '4grams_Topic introduction',\n",
    " '4grams_Discourse organizers',\n",
    " '4grams_Quantifying(a great deal of)',\n",
    " '4grams_Quantifying(a great number of)',\n",
    " '4grams_Quantifying(a large amount of)',\n",
    " '4grams_Quantifying(a lot of people)',\n",
    " '4grams_Quantifying(a lot of problem)',\n",
    " '4grams_Quantifying(a lot of problems)',\n",
    " '4grams_Quantifying(a lot of time)',\n",
    " '4grams_Quantifying(all of them are)',\n",
    " '4grams_Quantifying(and a lot of)',\n",
    " '4grams_Quantifying(bring a lot of)',\n",
    " '4grams_Quantifying(has a lot of)',\n",
    " '4grams_Quantifying(more and more people)',\n",
    " '4grams_Quantifying(most of the people)',\n",
    " '4grams_Quantifying(most of them are)',\n",
    " '4grams_Quantifying(some of them are)',\n",
    " '4grams_Quantifying(that it is more)',\n",
    " '4grams_Quantifying(the rest of the)',\n",
    " '4grams_Quantifying(the rest of the world)',\n",
    " '4grams_Quantifying(there are a lot of)',\n",
    " '4grams_Quantifying(there are many people)',\n",
    " '4grams_Quantifying(there are quite a)',\n",
    " '4grams_Quantifying(there are so many)',\n",
    " '4grams_Quantifying(there are still some)',\n",
    " '4grams_Quantifying(there are too many)',\n",
    " '4grams_Quantifying(with a lot of)',\n",
    " '4grams_Quantifying',\n",
    " '4grams_Time/place/text deixis(all over the world)',\n",
    " '4grams_Time/place/text deixis(at the beginning of the)',\n",
    " '4grams_Time/place/text deixis(at the same time)',\n",
    " '4grams_Time/place/text deixis(for a long time)',\n",
    " '4grams_Time/place/text deixis(in the following paragraphs)',\n",
    " '4grams_Time/place/text deixis(the end of the)',\n",
    " '4grams_Time/place/text deixis',\n",
    " '4grams_Framing(because they are not)',\n",
    " '4grams_Framing(in such a way)',\n",
    " '4grams_Framing(in the process of)',\n",
    " '4grams_Framing(on the basis of)',\n",
    " '4grams_Framing(the main reason is)',\n",
    " '4grams_Framing(the quality of the)',\n",
    " '4grams_Framing(the reason is that)',\n",
    " '4grams_Framing(the relationship between the)',\n",
    " '4grams_Framing(the result of the)',\n",
    " '4grams_Framing(with the development of)',\n",
    " '4grams_Framing(as a result of)',\n",
    " '4grams_Framing(as the result of)',\n",
    " '4grams_Framing(the result of this)',\n",
    " '4grams_Framing',\n",
    " '4grams_Referential',\n",
    " '4grams_Epistemic(as a matter of)',\n",
    " '4grams_Epistemic(as we all know)',\n",
    " '4grams_Epistemic(become more and more)',\n",
    " '4grams_Epistemic(I think it is)',\n",
    " '4grams_Epistemic(I think that this)',\n",
    " '4grams_Epistemic(I think the most)',\n",
    " '4grams_Epistemic(I think this is)',\n",
    " '4grams_Epistemic(it is believed that)',\n",
    " '4grams_Epistemic(it is obvious that)',\n",
    " '4grams_Epistemic(it is true that)',\n",
    " '4grams_Epistemic(some people think that)',\n",
    " '4grams_Epistemic',\n",
    " '4grams_Attitudinal/modality(are not allowed to)',\n",
    " '4grams_Attitudinal/modality(I hope I can)',\n",
    " '4grams_Attitudinal/modality(is very important to)',\n",
    " '4grams_Attitudinal/modality(it is difficult to)',\n",
    " '4grams_Attitudinal/modality(it is very difficult)',\n",
    " '4grams_Attitudinal/modality(it is hard to)',\n",
    " '4grams_Attitudinal/modality(it is not easy)',\n",
    " '4grams_Attitudinal/modality(necessary for us to)',\n",
    " '4grams_Attitudinal/modality(should learn how to)',\n",
    " '4grams_Attitudinal/modality(will not be able to)',\n",
    " '4grams_Attitudinal/modality',\n",
    " '4grams_Stance',\n",
    " '4grams_all',\n",
    " 'num_4grams', 'index']\n",
    "lex_criteria = pd.read_csv('/Users/irene/Desktop/Диплом/code/result_criteria/rhet_criteria.csv',\n",
    "                           delimiter=',', names=head_lex, index_col=None)\n",
    "lex_criteria = lex_criteria.drop(['index'], axis=1)\n",
    "lex_criteria['Type' ] = df['Type']\n",
    "lex_criteria['Class'] = df['Class']\n",
    "lex_criteria.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = lex_criteria[lex_criteria['Class'] == 'best']\n",
    "worst = lex_criteria[lex_criteria['Class'] == 'worst']\n",
    "aver = lex_criteria[lex_criteria['Class'] == 'aver']\n",
    "best_1 = lex_criteria[(lex_criteria['Class'] == 'best') & (df['Type'] == 1)]\n",
    "worst_1 = lex_criteria[(lex_criteria['Class'] == 'worst') & (df['Type'] == 1)]\n",
    "aver_1 = lex_criteria[(lex_criteria['Class'] == 'aver') & (df['Type'] == 1)]\n",
    "best_2 = lex_criteria[(lex_criteria['Class'] == 'best') & (df['Type'] == 2)]\n",
    "worst_2 = lex_criteria[(lex_criteria['Class'] == 'worst') & (df['Type'] == 2)]\n",
    "aver_2 = lex_criteria[(lex_criteria['Class'] == 'aver') & (df['Type'] == 2)]\n",
    "first = lex_criteria[lex_criteria['Type'] == 1]\n",
    "second = lex_criteria[lex_criteria['Type'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criteria = ['4grams_all', 'num_4grams', 'link_all', '4grams_Stance', '4grams_Epistemic',\n",
    "            '4grams_Framing', '4grams_Referential', '4grams_Quantifying',  '4grams_Topic introduction', \n",
    "            '4grams_Discourse organizers', '4grams_Identification/focus',  \n",
    "            '4grams_Topic elaboration/clarification',  'link_Intensification',  \n",
    "            'link_Illustration', 'link_Contrast',  'link_Sentence connectors',\n",
    " 'link_Phrases linkers',\n",
    " 'link_Addition',\n",
    " 'link_subordinators',\n",
    " 'link_Phrase linkers',\n",
    " 'link_Adversativity',\n",
    " 'link_Cause and effect',\n",
    " 'link_Clarification',\n",
    " 'link_Subordinators'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Критерий: 4grams_all\n",
      "t = 0.74792\n",
      "p = 0.91256\n",
      "Критерий: num_4grams\n",
      "t = 1.24472\n",
      "p = 0.43235\n",
      "Критерий: link_all\n",
      "t = 2.90934\n",
      "p = 0.00895\n",
      "Статистически значимая разница по критерию 'link_all'\n",
      "Критерий: 4grams_Stance\n",
      "t = -0.24803\n",
      "p = 1.60925\n",
      "Критерий: 4grams_Epistemic\n",
      "t = -0.27167\n",
      "p = 1.57289\n",
      "Критерий: 4grams_Framing\n",
      "t = 0.25973\n",
      "p = 1.59122\n",
      "Критерий: 4grams_Referential\n",
      "t = 3.23327\n",
      "p = 0.00333\n",
      "Статистически значимая разница по критерию '4grams_Referential'\n",
      "Критерий: 4grams_Quantifying\n",
      "t = 3.77135\n",
      "p = 0.00055\n",
      "Статистически значимая разница по критерию '4grams_Quantifying'\n",
      "Критерий: 4grams_Topic introduction\n",
      "t = 0.93852\n",
      "p = 0.70053\n",
      "Критерий: 4grams_Discourse organizers\n",
      "t = -0.48428\n",
      "p = 1.25852\n",
      "Критерий: 4grams_Identification/focus\n",
      "t = -0.65857\n",
      "p = 1.0234\n",
      "Критерий: 4grams_Topic elaboration/clarification\n",
      "t = -0.75826\n",
      "p = 0.9002\n",
      "Критерий: link_Intensification\n",
      "t = 1.73259\n",
      "p = 0.17257\n",
      "Критерий: link_Illustration\n",
      "t = -0.08644\n",
      "p = 1.86259\n",
      "Критерий: link_Contrast\n",
      "t = 0.95691\n",
      "p = 0.68189\n",
      "Критерий: link_Sentence connectors\n",
      "t = 1.73259\n",
      "p = 0.17257\n",
      "Критерий: link_Phrases linkers\n",
      "t = -0.85741\n",
      "p = 0.78658\n",
      "Критерий: link_Addition\n",
      "t = -0.23338\n",
      "p = 1.63189\n",
      "Критерий: link_subordinators\n",
      "t = 0.91532\n",
      "p = 0.7245\n",
      "Критерий: link_Phrase linkers\n",
      "t = nan\n",
      "p = nan\n",
      "Критерий: link_Adversativity\n",
      "t = 1.8987\n",
      "p = 0.12103\n",
      "Критерий: link_Cause and effect\n",
      "t = 2.34154\n",
      "p = 0.04242\n",
      "Статистически значимая разница по критерию 'link_Cause and effect'\n",
      "Критерий: link_Clarification\n",
      "t = 2.75725\n",
      "p = 0.01388\n",
      "Статистически значимая разница по критерию 'link_Clarification'\n",
      "Критерий: link_Subordinators\n",
      "t = 0.72704\n",
      "p = 0.93783\n"
     ]
    }
   ],
   "source": [
    "for c in criteria:\n",
    "    t, p = stats.ttest_ind(best[c],worst[c])\n",
    "    print('Критерий:', c)\n",
    "    print(\"t = \" + str(round(t, 5)))\n",
    "    print(\"p = \" + str(round(2*p, 5)))\n",
    "    if 2*p < 0.05:\n",
    "        print(\"Статистически значимая разница по критерию '%s'\" % c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
