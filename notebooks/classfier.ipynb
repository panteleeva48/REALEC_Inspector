{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ira/Downloads/diplom/REALEC_Inspector')\n",
    "\n",
    "import sqlite3\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from utils.main import main\n",
    "import pandas as pd\n",
    "import collections\n",
    "import statistics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import json\n",
    "with open('result.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "\n",
    "SEED = 23\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(''))\n",
    "PATH_DB = os.path.join(BASE_DIR, 'data', 'realec.db')\n",
    "JSON_FILE = os.path.join(BASE_DIR, 'data', 'files_with_json.txt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(dbfile):\n",
    "    conn = sqlite3.connect(dbfile)\n",
    "    c = conn.cursor()\n",
    "    return c, conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = {'density': 0.43564356435643564, 'LS': 0.14772727272727273, 'VSI': 0.11764705882352941, 'VSII': 0.34299717028501764, 'VSIII': 0.23529411764705882, 'LFP_first_procent': 0.5643564356435643, 'LFP_second_procent': 0.06930693069306931, 'LFP_third_procent': 0.054455445544554455, 'LFP_none': 0.31188118811881194, 'NDW': 103, 'TTR': 0.5099009900990099, 'CTTR': 5.1244415295814445, 'RTTR': 7.247054710722006, 'LogTTR': 0.8731151577940734, 'Uber': 287.17064749454755, 'D': 0.8129560915759496, 'LV': 0.5099009900990099, 'VVI': 0.8823529411764706, 'SVVI': 13.235294117647058, 'CVVI': 2.5724787771376323, 'VVII': 0.17045454545454544, 'NV': 0.1188118811881188, 'AdjV': 0.17045454545454544, 'AdvV': 0.09090909090909091, 'ModV': 0.26136363636363635, 'der_suff3': 0.0, 'der_suff4': 0.1590909090909091, 'der_suff5': 0.022727272727272728, 'der_suff6': 0.0, 'MCI': 5.25, 'freq_finite_forms': 0.47058823529411764, 'freq_aux': 0.4117647058823529, 'infinitive_tokens': 4, 'gerund_tokens': 0, 'pres_sg_tokens': 9, 'pres_pl_tokens': 0, 'parts': 3, 'pasts': 1, 'link_Sentence connectors(Furthermore)': 0, 'link_Sentence connectors(In addition)': 0, 'link_Sentence connectors(Moreover)': 0, 'link_Sentence connectors': 0, 'link_Phrases linkers(In addition)': 0, 'link_Phrases linkers': 0, 'link_Addition': 0, 'link_subordinators(Although)': 0, 'link_subordinators(Even though)': 0, 'link_subordinators': 0, 'link_Sentence connectors(However)': 0, 'link_Sentence connectors(Nevertheless)': 0, 'link_Phrase linkers(despite)': 0, 'link_Phrase linkers(In spite of)': 0, 'link_Phrase linkers': 0, 'link_Adversativity': 0, 'link_subordinators(Because)': 0, 'link_subordinators(since)': 0, 'link_Sentence connectors(Therefore)': 0, 'link_Sentence connectors(As a result)': 0, 'link_Sentence connectors(consequently)': 0, 'link_Sentence connectors(Hence)': 0, 'link_Sentence connectors(Thus)': 0, 'link_Phrase linkers(Because of)': 0, 'link_Phrase linkers(Due to)': 0, 'link_Phrase linkers(As a result of)': 0, 'link_Cause and effect': 0, 'link_Sentence connectors(In other words)': 0, 'link_Sentence connectors(That is)': 0, 'link_Sentence connectors(i.e.)': 4, 'link_Clarification': 4, 'link_Subordinators(while)': 0, 'link_Subordinators(whereas)': 0, 'link_Subordinators': 0, 'link_Sentence connectors(In contrast)': 0, 'link_Sentence connectors(On the other hand)': 0, 'link_Sentence connectors(Conversely)': 0, 'link_Phrase linkers(Unlike)': 0, 'link_Contrast': 0, 'link_Sentence connectors(For example)': 0, 'link_Sentence connectors(For instance)': 0, 'link_Illustration': 0, 'link_Sentence connectors(On the contrary)': 0, 'link_Sentence connectors(As a matter of fact)': 0, 'link_Sentence connectors(In fact)': 0, 'link_Intensification': 0, 'link_all': 4, 'num_4grams': 1, '4grams_Topic elaboration/clarification(and to be a)': 0, '4grams_Topic elaboration/clarification(are more and more)': 0, '4grams_Topic elaboration/clarification(as well as the)': 0, '4grams_Topic elaboration/clarification(but there are still)': 0, '4grams_Topic elaboration/clarification(can be divided into)': 0, '4grams_Topic elaboration/clarification(how to deal with)': 0, '4grams_Topic elaboration/clarification(if you don’t know)': 0, '4grams_Topic elaboration/clarification(if you want to)': 0, '4grams_Topic elaboration/clarification(in order to make)': 0, '4grams_Topic elaboration/clarification(is a kind of)': 0, '4grams_Topic elaboration/clarification(is based on the)': 0, '4grams_Topic elaboration/clarification(is more important than)': 0, '4grams_Topic elaboration/clarification(is totally different from)': 0, '4grams_Topic elaboration/clarification(it is a good)': 0, '4grams_Topic elaboration/clarification(it is a very)': 0, '4grams_Topic elaboration/clarification(it is also a)': 0, '4grams_Topic elaboration/clarification(it is because the)': 0, '4grams_Topic elaboration/clarification(it is not a)': 0, '4grams_Topic elaboration/clarification(on the other hand)': 0, '4grams_Topic elaboration/clarification(there will be a)': 0, '4grams_Topic elaboration/clarification(to cope with the)': 0, '4grams_Topic elaboration/clarification(want to be a)': 0, '4grams_Topic elaboration/clarification': 0, '4grams_Identification/focus(my point of view)': 0, '4grams_Identification/focus(the best way to)': 0, '4grams_Identification/focus(a very important role)': 0, '4grams_Identification/focus(as far as the)': 0, '4grams_Identification/focus(as I have mentioned)': 0, '4grams_Identification/focus(him or her to)': 0, '4grams_Identification/focus(is one of my)': 0, '4grams_Identification/focus(is one of the)': 0, '4grams_Identification/focus(is the most important)': 0, '4grams_Identification/focus(is very important for)': 0, '4grams_Identification/focus(it is very important)': 0, '4grams_Identification/focus(one of the most)': 0, '4grams_Identification/focus(the most important thing)': 0, '4grams_Identification/focus(we can say that)': 0, '4grams_Identification/focus(we can see that)': 0, '4grams_Identification/focus(we can see the)': 0, '4grams_Identification/focus': 0, '4grams_Topic introduction(I am going to)': 0, '4grams_Topic introduction(I would like to)': 0, '4grams_Topic introduction(if there is a)': 0, '4grams_Topic introduction': 0, '4grams_Discourse organizers': 0, '4grams_Quantifying(a great deal of)': 0, '4grams_Quantifying(a great number of)': 0, '4grams_Quantifying(a large amount of)': 0, '4grams_Quantifying(a lot of people)': 0, '4grams_Quantifying(a lot of problem)': 0, '4grams_Quantifying(a lot of problems)': 0, '4grams_Quantifying(a lot of time)': 0, '4grams_Quantifying(all of them are)': 0, '4grams_Quantifying(and a lot of)': 0, '4grams_Quantifying(bring a lot of)': 0, '4grams_Quantifying(has a lot of)': 0, '4grams_Quantifying(more and more people)': 0, '4grams_Quantifying(most of the people)': 0, '4grams_Quantifying(most of them are)': 0, '4grams_Quantifying(some of them are)': 0, '4grams_Quantifying(that it is more)': 0, '4grams_Quantifying(the rest of the)': 0, '4grams_Quantifying(the rest of the world)': 0, '4grams_Quantifying(there are a lot of)': 0, '4grams_Quantifying(there are many people)': 0, '4grams_Quantifying(there are quite a)': 0, '4grams_Quantifying(there are so many)': 0, '4grams_Quantifying(there are still some)': 0, '4grams_Quantifying(there are too many)': 0, '4grams_Quantifying(with a lot of)': 0, '4grams_Quantifying': 0, '4grams_Time/place/text deixis(all over the world)': 0, '4grams_Time/place/text deixis(at the beginning of the)': 0, '4grams_Time/place/text deixis(at the same time)': 0, '4grams_Time/place/text deixis(for a long time)': 0, '4grams_Time/place/text deixis(in the following paragraphs)': 0, '4grams_Time/place/text deixis(the end of the)': 0, '4grams_Time/place/text deixis': 0, '4grams_Framing(because they are not)': 0, '4grams_Framing(in such a way)': 0, '4grams_Framing(in the process of)': 0, '4grams_Framing(on the basis of)': 0, '4grams_Framing(the main reason is)': 0, '4grams_Framing(the quality of the)': 0, '4grams_Framing(the reason is that)': 0, '4grams_Framing(the relationship between the)': 0, '4grams_Framing(the result of the)': 0, '4grams_Framing(with the development of)': 0, '4grams_Framing(as a result of)': 0, '4grams_Framing(as the result of)': 0, '4grams_Framing(the result of this)': 0, '4grams_Framing': 0, '4grams_Referential': 0, '4grams_Epistemic(as a matter of)': 0, '4grams_Epistemic(as we all know)': 0, '4grams_Epistemic(become more and more)': 0, '4grams_Epistemic(I think it is)': 0, '4grams_Epistemic(I think that this)': 0, '4grams_Epistemic(I think the most)': 0, '4grams_Epistemic(I think this is)': 0, '4grams_Epistemic(it is believed that)': 0, '4grams_Epistemic(it is obvious that)': 0, '4grams_Epistemic(it is true that)': 0, '4grams_Epistemic(some people think that)': 0, '4grams_Epistemic': 0, '4grams_Attitudinal/modality(are not allowed to)': 0, '4grams_Attitudinal/modality(I hope I can)': 0, '4grams_Attitudinal/modality(is very important to)': 0, '4grams_Attitudinal/modality(it is difficult to)': 0, '4grams_Attitudinal/modality(it is very difficult)': 0, '4grams_Attitudinal/modality(it is hard to)': 0, '4grams_Attitudinal/modality(it is not easy)': 0, '4grams_Attitudinal/modality(necessary for us to)': 0, '4grams_Attitudinal/modality(should learn how to)': 0, '4grams_Attitudinal/modality(will not be able to)': 0, '4grams_Attitudinal/modality': 0, '4grams_Stance': 0, '4grams_all': 0, 'av_depth': 3.5384615384615383, 'max_depth': 5, 'min_depth': 2, 'acl': 4, 'rel_cl': 2, 'advcl': 0, 'count_sent': 13, 'count_tokens': 202, 'tokens_before_root': 3.1538461538461537, 'mean_len_sent': 15.538461538461538}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = {}\n",
    "# for x in xx:\n",
    "#     new[x] = []\n",
    "# new['name'] = []\n",
    "# new['text'] = []\n",
    "# new['target'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# import io\n",
    "# try:\n",
    "#     to_unicode = unicode\n",
    "# except NameError:\n",
    "#     to_unicode = str\n",
    "\n",
    "# # Write JSON file\n",
    "# with io.open('result.json', 'w', encoding='utf8') as outfile:\n",
    "#     str_ = json.dumps(new,\n",
    "#                       indent=4, sort_keys=True,\n",
    "#                       separators=(',', ': '), ensure_ascii=False)\n",
    "#     outfile.write(to_unicode(str_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_FILE, 'r') as rf:\n",
    "    clean_essays = rf.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b78c69ff714154a4973385a14e49fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3446), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c, conn = connect(PATH_DB)\n",
    "c.execute(\"SELECT TEXT, MARK, NAME FROM MAIN\")\n",
    "result = c.fetchall()\n",
    "for essay in tqdm(result):\n",
    "    text = essay[0]\n",
    "    mark = essay[1]\n",
    "    name = essay[2]\n",
    "    if name in clean_essays:\n",
    "        try:\n",
    "            result = main(text)\n",
    "            for key in result:\n",
    "                data[key].append(result[key])\n",
    "            data['name'].append(name)\n",
    "            data['text'].append(text)\n",
    "            data['target'].append(mark)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data)\n",
    "df.head()\n",
    "df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4grams_Attitudinal/modality</th>\n",
       "      <th>4grams_Attitudinal/modality(I hope I can)</th>\n",
       "      <th>4grams_Attitudinal/modality(are not allowed to)</th>\n",
       "      <th>4grams_Attitudinal/modality(is very important to)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is difficult to)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is hard to)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is not easy)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is very difficult)</th>\n",
       "      <th>4grams_Attitudinal/modality(necessary for us to)</th>\n",
       "      <th>4grams_Attitudinal/modality(should learn how to)</th>\n",
       "      <th>...</th>\n",
       "      <th>name</th>\n",
       "      <th>num_4grams</th>\n",
       "      <th>parts</th>\n",
       "      <th>pasts</th>\n",
       "      <th>pres_pl_tokens</th>\n",
       "      <th>pres_sg_tokens</th>\n",
       "      <th>rel_cl</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_before_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>./data/exam/exam2017/OBy_146_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>This two line graphs illustrates monthly avera...</td>\n",
       "      <td>4.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>./data/exam/exam2017/EGe_15_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>﻿We have two graphics, which show us the popul...</td>\n",
       "      <td>6.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>./data/exam/exam2017/DOv_2_2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>More and more young people are stunding on the...</td>\n",
       "      <td>4.562500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   4grams_Attitudinal/modality  4grams_Attitudinal/modality(I hope I can)  \\\n",
       "0                            0                                          0   \n",
       "1                            0                                          0   \n",
       "2                            1                                          0   \n",
       "\n",
       "   4grams_Attitudinal/modality(are not allowed to)  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "\n",
       "   4grams_Attitudinal/modality(is very important to)  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  1   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is difficult to)  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is hard to)  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is not easy)  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is very difficult)  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "\n",
       "   4grams_Attitudinal/modality(necessary for us to)  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "\n",
       "   4grams_Attitudinal/modality(should learn how to)  ...  \\\n",
       "0                                                 0  ...   \n",
       "1                                                 0  ...   \n",
       "2                                                 0  ...   \n",
       "\n",
       "                             name  num_4grams  parts  pasts  pres_pl_tokens  \\\n",
       "0  ./data/exam/exam2017/OBy_146_1           1      2      0               0   \n",
       "1   ./data/exam/exam2017/EGe_15_1           1      1      4               0   \n",
       "2    ./data/exam/exam2017/DOv_2_2           5      2      9               0   \n",
       "\n",
       "   pres_sg_tokens  rel_cl  target  \\\n",
       "0               4       0      60   \n",
       "1               5       3      65   \n",
       "2               5       0      60   \n",
       "\n",
       "                                                text  tokens_before_root  \n",
       "0  This two line graphs illustrates monthly avera...            4.388889  \n",
       "1  ﻿We have two graphics, which show us the popul...            6.285714  \n",
       "2  More and more young people are stunding on the...            4.562500  \n",
       "\n",
       "[3 rows x 217 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3399, 217)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(data.keys())\n",
    "feature_columns.remove('target')\n",
    "feature_columns.remove('name')\n",
    "feature_columns.remove('text')\n",
    "feature_columns = feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer('I am Ira')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'average': 2068, 'worst': 950, 'best': 381})\n"
     ]
    }
   ],
   "source": [
    "# tfidf = TfidfVectorizer(lowercase=True, tokenizer=tokenizer)\n",
    "X = df[feature_columns]\n",
    "targets = []\n",
    "for t in df['target']:\n",
    "    if int(t) >= 70:\n",
    "        targets.append('best')\n",
    "    elif int(t) < 60:\n",
    "        targets.append('worst')\n",
    "    else:\n",
    "        targets.append('average')\n",
    "y = targets\n",
    "\n",
    "counter=collections.Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_numeric_data = FunctionTransformer(lambda x: x[feature_columns], validate=False)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "            ('selector', get_numeric_data)\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', LinearSVC(class_weight='balanced', random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, model, kf):\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    dicts = []\n",
    "    \n",
    "    for train_index, test_index in tqdm(kf.split(X, y)):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train = pd.DataFrame(data=X_train[0:,0:],\n",
    "                               columns=[feature_columns])\n",
    "        X_test = pd.DataFrame(data=X_test[0:,0:],\n",
    "                               columns=[feature_columns])\n",
    "\n",
    "        X_train[feature_columns] = X_train[feature_columns].apply(pd.to_numeric)\n",
    "        X_test[feature_columns] = X_test[feature_columns].apply(pd.to_numeric)\n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        dicts.append(report)\n",
    "    \n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172ac162423244dc8f9331d0a9d2f077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "dicts = cross_val(X, y, model, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['best', 'worst', 'average', \n",
    "           'micro avg', 'macro avg', 'weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.506</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.489</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.596</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score\n",
       "best              0.263   0.213     0.123\n",
       "worst             0.506   0.548     0.416\n",
       "average           0.699   0.498     0.477\n",
       "micro avg         0.480   0.480     0.480\n",
       "macro avg         0.489   0.420     0.339\n",
       "weighted avg      0.596   0.480     0.420"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\n",
    "    'precision': [],\n",
    "    'recall': [], \n",
    "    'f1-score': []\n",
    "}\n",
    "\n",
    "for cl in classes:\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "    for d in dicts:\n",
    "        precision.append(d[cl]['precision'])\n",
    "        recall.append(d[cl]['recall'])\n",
    "        f1_score.append(d[cl]['f1-score'])\n",
    "    result['precision'].append(round(statistics.mean(precision), 3))\n",
    "    result['recall'].append(round(statistics.mean(recall), 3))\n",
    "    result['f1-score'].append(round(statistics.mean(f1_score),3))\n",
    "result = pd.DataFrame(data=result, index=classes)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BorutaPy(alpha=0.05,\n",
       "     estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=5, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=187, n_jobs=-1, oob_score=False,\n",
       "            random_state=<mtrand.RandomState object at 0x10584e048>,\n",
       "            verbose=0, warm_start=False),\n",
       "     max_iter=100, n_estimators='auto', perc=100,\n",
       "     random_state=<mtrand.RandomState object at 0x10584e048>,\n",
       "     two_step=True, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=SEED)\n",
    "feat_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdvV\n",
      "CTTR\n",
      "CVVI\n",
      "D\n",
      "LFP_first_procent\n",
      "LFP_none\n",
      "LFP_third_procent\n",
      "LS\n",
      "LV\n",
      "LogTTR\n",
      "MCI\n",
      "ModV\n",
      "NDW\n",
      "NV\n",
      "RTTR\n",
      "SVVI\n",
      "TTR\n",
      "VVII\n",
      "acl\n",
      "advcl\n",
      "av_depth\n",
      "count_sent\n",
      "count_tokens\n",
      "density\n",
      "der_suff3\n",
      "der_suff4\n",
      "der_suff6\n",
      "freq_finite_forms\n",
      "gerund_tokens\n",
      "infinitive_tokens\n",
      "link_Clarification\n",
      "link_Contrast\n",
      "link_Sentence connectors(i.e.)\n",
      "link_Subordinators\n",
      "link_Subordinators(while)\n",
      "link_all\n",
      "max_depth\n",
      "mean_len_sent\n",
      "num_4grams\n",
      "parts\n",
      "pres_sg_tokens\n",
      "tokens_before_root\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(feat_selector.support_):\n",
    "    if feature == True:\n",
    "        print(feature_columns[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "REALEC_Inspector",
   "language": "python",
   "name": "realec_inspector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
