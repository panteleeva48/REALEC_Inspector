{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ira/Downloads/diplom/REALEC_Inspector')\n",
    "\n",
    "import sqlite3\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from utils.main import main\n",
    "import pandas as pd\n",
    "import collections\n",
    "import statistics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import json\n",
    "with open('result.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "\n",
    "SEED = 23\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(''))\n",
    "PATH_DB = os.path.join(BASE_DIR, 'data', 'realec.db')\n",
    "JSON_FILE = os.path.join(BASE_DIR, 'data', 'files_with_json.txt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(dbfile):\n",
    "    conn = sqlite3.connect(dbfile)\n",
    "    c = conn.cursor()\n",
    "    return c, conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = {'density': 0.43564356435643564, 'LS': 0.14772727272727273, 'VSI': 0.11764705882352941, 'VSII': 0.34299717028501764, 'VSIII': 0.23529411764705882, 'LFP_first_procent': 0.5643564356435643, 'LFP_second_procent': 0.06930693069306931, 'LFP_third_procent': 0.054455445544554455, 'LFP_none': 0.31188118811881194, 'NDW': 103, 'TTR': 0.5099009900990099, 'CTTR': 5.1244415295814445, 'RTTR': 7.247054710722006, 'LogTTR': 0.8731151577940734, 'Uber': 287.17064749454755, 'D': 0.8129560915759496, 'LV': 0.5099009900990099, 'VVI': 0.8823529411764706, 'SVVI': 13.235294117647058, 'CVVI': 2.5724787771376323, 'VVII': 0.17045454545454544, 'NV': 0.1188118811881188, 'AdjV': 0.17045454545454544, 'AdvV': 0.09090909090909091, 'ModV': 0.26136363636363635, 'der_suff3': 0.0, 'der_suff4': 0.1590909090909091, 'der_suff5': 0.022727272727272728, 'der_suff6': 0.0, 'MCI': 5.25, 'freq_finite_forms': 0.47058823529411764, 'freq_aux': 0.4117647058823529, 'infinitive_tokens': 4, 'gerund_tokens': 0, 'pres_sg_tokens': 9, 'pres_pl_tokens': 0, 'parts': 3, 'pasts': 1, 'link_Sentence connectors(Furthermore)': 0, 'link_Sentence connectors(In addition)': 0, 'link_Sentence connectors(Moreover)': 0, 'link_Sentence connectors': 0, 'link_Phrases linkers(In addition)': 0, 'link_Phrases linkers': 0, 'link_Addition': 0, 'link_subordinators(Although)': 0, 'link_subordinators(Even though)': 0, 'link_subordinators': 0, 'link_Sentence connectors(However)': 0, 'link_Sentence connectors(Nevertheless)': 0, 'link_Phrase linkers(despite)': 0, 'link_Phrase linkers(In spite of)': 0, 'link_Phrase linkers': 0, 'link_Adversativity': 0, 'link_subordinators(Because)': 0, 'link_subordinators(since)': 0, 'link_Sentence connectors(Therefore)': 0, 'link_Sentence connectors(As a result)': 0, 'link_Sentence connectors(consequently)': 0, 'link_Sentence connectors(Hence)': 0, 'link_Sentence connectors(Thus)': 0, 'link_Phrase linkers(Because of)': 0, 'link_Phrase linkers(Due to)': 0, 'link_Phrase linkers(As a result of)': 0, 'link_Cause and effect': 0, 'link_Sentence connectors(In other words)': 0, 'link_Sentence connectors(That is)': 0, 'link_Sentence connectors(i.e.)': 4, 'link_Clarification': 4, 'link_Subordinators(while)': 0, 'link_Subordinators(whereas)': 0, 'link_Subordinators': 0, 'link_Sentence connectors(In contrast)': 0, 'link_Sentence connectors(On the other hand)': 0, 'link_Sentence connectors(Conversely)': 0, 'link_Phrase linkers(Unlike)': 0, 'link_Contrast': 0, 'link_Sentence connectors(For example)': 0, 'link_Sentence connectors(For instance)': 0, 'link_Illustration': 0, 'link_Sentence connectors(On the contrary)': 0, 'link_Sentence connectors(As a matter of fact)': 0, 'link_Sentence connectors(In fact)': 0, 'link_Intensification': 0, 'link_all': 4, 'num_4grams': 1, '4grams_Topic elaboration/clarification(and to be a)': 0, '4grams_Topic elaboration/clarification(are more and more)': 0, '4grams_Topic elaboration/clarification(as well as the)': 0, '4grams_Topic elaboration/clarification(but there are still)': 0, '4grams_Topic elaboration/clarification(can be divided into)': 0, '4grams_Topic elaboration/clarification(how to deal with)': 0, '4grams_Topic elaboration/clarification(if you donâ€™t know)': 0, '4grams_Topic elaboration/clarification(if you want to)': 0, '4grams_Topic elaboration/clarification(in order to make)': 0, '4grams_Topic elaboration/clarification(is a kind of)': 0, '4grams_Topic elaboration/clarification(is based on the)': 0, '4grams_Topic elaboration/clarification(is more important than)': 0, '4grams_Topic elaboration/clarification(is totally different from)': 0, '4grams_Topic elaboration/clarification(it is a good)': 0, '4grams_Topic elaboration/clarification(it is a very)': 0, '4grams_Topic elaboration/clarification(it is also a)': 0, '4grams_Topic elaboration/clarification(it is because the)': 0, '4grams_Topic elaboration/clarification(it is not a)': 0, '4grams_Topic elaboration/clarification(on the other hand)': 0, '4grams_Topic elaboration/clarification(there will be a)': 0, '4grams_Topic elaboration/clarification(to cope with the)': 0, '4grams_Topic elaboration/clarification(want to be a)': 0, '4grams_Topic elaboration/clarification': 0, '4grams_Identification/focus(my point of view)': 0, '4grams_Identification/focus(the best way to)': 0, '4grams_Identification/focus(a very important role)': 0, '4grams_Identification/focus(as far as the)': 0, '4grams_Identification/focus(as I have mentioned)': 0, '4grams_Identification/focus(him or her to)': 0, '4grams_Identification/focus(is one of my)': 0, '4grams_Identification/focus(is one of the)': 0, '4grams_Identification/focus(is the most important)': 0, '4grams_Identification/focus(is very important for)': 0, '4grams_Identification/focus(it is very important)': 0, '4grams_Identification/focus(one of the most)': 0, '4grams_Identification/focus(the most important thing)': 0, '4grams_Identification/focus(we can say that)': 0, '4grams_Identification/focus(we can see that)': 0, '4grams_Identification/focus(we can see the)': 0, '4grams_Identification/focus': 0, '4grams_Topic introduction(I am going to)': 0, '4grams_Topic introduction(I would like to)': 0, '4grams_Topic introduction(if there is a)': 0, '4grams_Topic introduction': 0, '4grams_Discourse organizers': 0, '4grams_Quantifying(a great deal of)': 0, '4grams_Quantifying(a great number of)': 0, '4grams_Quantifying(a large amount of)': 0, '4grams_Quantifying(a lot of people)': 0, '4grams_Quantifying(a lot of problem)': 0, '4grams_Quantifying(a lot of problems)': 0, '4grams_Quantifying(a lot of time)': 0, '4grams_Quantifying(all of them are)': 0, '4grams_Quantifying(and a lot of)': 0, '4grams_Quantifying(bring a lot of)': 0, '4grams_Quantifying(has a lot of)': 0, '4grams_Quantifying(more and more people)': 0, '4grams_Quantifying(most of the people)': 0, '4grams_Quantifying(most of them are)': 0, '4grams_Quantifying(some of them are)': 0, '4grams_Quantifying(that it is more)': 0, '4grams_Quantifying(the rest of the)': 0, '4grams_Quantifying(the rest of the world)': 0, '4grams_Quantifying(there are a lot of)': 0, '4grams_Quantifying(there are many people)': 0, '4grams_Quantifying(there are quite a)': 0, '4grams_Quantifying(there are so many)': 0, '4grams_Quantifying(there are still some)': 0, '4grams_Quantifying(there are too many)': 0, '4grams_Quantifying(with a lot of)': 0, '4grams_Quantifying': 0, '4grams_Time/place/text deixis(all over the world)': 0, '4grams_Time/place/text deixis(at the beginning of the)': 0, '4grams_Time/place/text deixis(at the same time)': 0, '4grams_Time/place/text deixis(for a long time)': 0, '4grams_Time/place/text deixis(in the following paragraphs)': 0, '4grams_Time/place/text deixis(the end of the)': 0, '4grams_Time/place/text deixis': 0, '4grams_Framing(because they are not)': 0, '4grams_Framing(in such a way)': 0, '4grams_Framing(in the process of)': 0, '4grams_Framing(on the basis of)': 0, '4grams_Framing(the main reason is)': 0, '4grams_Framing(the quality of the)': 0, '4grams_Framing(the reason is that)': 0, '4grams_Framing(the relationship between the)': 0, '4grams_Framing(the result of the)': 0, '4grams_Framing(with the development of)': 0, '4grams_Framing(as a result of)': 0, '4grams_Framing(as the result of)': 0, '4grams_Framing(the result of this)': 0, '4grams_Framing': 0, '4grams_Referential': 0, '4grams_Epistemic(as a matter of)': 0, '4grams_Epistemic(as we all know)': 0, '4grams_Epistemic(become more and more)': 0, '4grams_Epistemic(I think it is)': 0, '4grams_Epistemic(I think that this)': 0, '4grams_Epistemic(I think the most)': 0, '4grams_Epistemic(I think this is)': 0, '4grams_Epistemic(it is believed that)': 0, '4grams_Epistemic(it is obvious that)': 0, '4grams_Epistemic(it is true that)': 0, '4grams_Epistemic(some people think that)': 0, '4grams_Epistemic': 0, '4grams_Attitudinal/modality(are not allowed to)': 0, '4grams_Attitudinal/modality(I hope I can)': 0, '4grams_Attitudinal/modality(is very important to)': 0, '4grams_Attitudinal/modality(it is difficult to)': 0, '4grams_Attitudinal/modality(it is very difficult)': 0, '4grams_Attitudinal/modality(it is hard to)': 0, '4grams_Attitudinal/modality(it is not easy)': 0, '4grams_Attitudinal/modality(necessary for us to)': 0, '4grams_Attitudinal/modality(should learn how to)': 0, '4grams_Attitudinal/modality(will not be able to)': 0, '4grams_Attitudinal/modality': 0, '4grams_Stance': 0, '4grams_all': 0, 'av_depth': 3.5384615384615383, 'max_depth': 5, 'min_depth': 2, 'acl': 4, 'rel_cl': 2, 'advcl': 0, 'count_sent': 13, 'count_tokens': 202, 'tokens_before_root': 3.1538461538461537, 'mean_len_sent': 15.538461538461538}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = {}\n",
    "# for x in xx:\n",
    "#     new[x] = []\n",
    "# new['name'] = []\n",
    "# new['text'] = []\n",
    "# new['target'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# import io\n",
    "# try:\n",
    "#     to_unicode = unicode\n",
    "# except NameError:\n",
    "#     to_unicode = str\n",
    "\n",
    "# # Write JSON file\n",
    "# with io.open('result.json', 'w', encoding='utf8') as outfile:\n",
    "#     str_ = json.dumps(new,\n",
    "#                       indent=4, sort_keys=True,\n",
    "#                       separators=(',', ': '), ensure_ascii=False)\n",
    "#     outfile.write(to_unicode(str_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_FILE, 'r') as rf:\n",
    "    clean_essays = rf.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b78c69ff714154a4973385a14e49fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3446), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c, conn = connect(PATH_DB)\n",
    "c.execute(\"SELECT TEXT, MARK, NAME FROM MAIN\")\n",
    "result = c.fetchall()\n",
    "for essay in tqdm(result):\n",
    "    text = essay[0]\n",
    "    mark = essay[1]\n",
    "    name = essay[2]\n",
    "    if name in clean_essays:\n",
    "        try:\n",
    "            result = main(text)\n",
    "            for key in result:\n",
    "                data[key].append(result[key])\n",
    "            data['name'].append(name)\n",
    "            data['text'].append(text)\n",
    "            data['target'].append(mark)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data)\n",
    "df.head()\n",
    "df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4grams_Attitudinal/modality</th>\n",
       "      <th>4grams_Attitudinal/modality(I hope I can)</th>\n",
       "      <th>4grams_Attitudinal/modality(are not allowed to)</th>\n",
       "      <th>4grams_Attitudinal/modality(is very important to)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is difficult to)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is hard to)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is not easy)</th>\n",
       "      <th>4grams_Attitudinal/modality(it is very difficult)</th>\n",
       "      <th>4grams_Attitudinal/modality(necessary for us to)</th>\n",
       "      <th>4grams_Attitudinal/modality(should learn how to)</th>\n",
       "      <th>...</th>\n",
       "      <th>name</th>\n",
       "      <th>num_4grams</th>\n",
       "      <th>parts</th>\n",
       "      <th>pasts</th>\n",
       "      <th>pres_pl_tokens</th>\n",
       "      <th>pres_sg_tokens</th>\n",
       "      <th>rel_cl</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_before_root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>./data/exam/exam2017/OBy_146_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>This two line graphs illustrates monthly avera...</td>\n",
       "      <td>4.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>./data/exam/exam2017/EGe_15_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>ï»¿We have two graphics, which show us the popul...</td>\n",
       "      <td>6.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>./data/exam/exam2017/DOv_2_2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>More and more young people are stunding on the...</td>\n",
       "      <td>4.562500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   4grams_Attitudinal/modality  4grams_Attitudinal/modality(I hope I can)  \\\n",
       "0                            0                                          0   \n",
       "1                            0                                          0   \n",
       "2                            1                                          0   \n",
       "\n",
       "   4grams_Attitudinal/modality(are not allowed to)  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "\n",
       "   4grams_Attitudinal/modality(is very important to)  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  1   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is difficult to)  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is hard to)  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is not easy)  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "2                                            0   \n",
       "\n",
       "   4grams_Attitudinal/modality(it is very difficult)  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "\n",
       "   4grams_Attitudinal/modality(necessary for us to)  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "\n",
       "   4grams_Attitudinal/modality(should learn how to)  ...  \\\n",
       "0                                                 0  ...   \n",
       "1                                                 0  ...   \n",
       "2                                                 0  ...   \n",
       "\n",
       "                             name  num_4grams  parts  pasts  pres_pl_tokens  \\\n",
       "0  ./data/exam/exam2017/OBy_146_1           1      2      0               0   \n",
       "1   ./data/exam/exam2017/EGe_15_1           1      1      4               0   \n",
       "2    ./data/exam/exam2017/DOv_2_2           5      2      9               0   \n",
       "\n",
       "   pres_sg_tokens  rel_cl  target  \\\n",
       "0               4       0      60   \n",
       "1               5       3      65   \n",
       "2               5       0      60   \n",
       "\n",
       "                                                text  tokens_before_root  \n",
       "0  This two line graphs illustrates monthly avera...            4.388889  \n",
       "1  ï»¿We have two graphics, which show us the popul...            6.285714  \n",
       "2  More and more young people are stunding on the...            4.562500  \n",
       "\n",
       "[3 rows x 217 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3399, 217)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(data.keys())\n",
    "feature_columns.remove('target')\n",
    "feature_columns.remove('name')\n",
    "feature_columns.remove('text')\n",
    "feature_columns = feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer('I am Ira')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'average': 2068, 'worst': 950, 'best': 381})\n"
     ]
    }
   ],
   "source": [
    "# tfidf = TfidfVectorizer(lowercase=True, tokenizer=tokenizer)\n",
    "X = df[feature_columns]\n",
    "targets = []\n",
    "for t in df['target']:\n",
    "    if int(t) >= 70:\n",
    "        targets.append('best')\n",
    "    elif int(t) < 60:\n",
    "        targets.append('worst')\n",
    "    else:\n",
    "        targets.append('average')\n",
    "y = targets\n",
    "\n",
    "counter=collections.Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_numeric_data = FunctionTransformer(lambda x: x[feature_columns], validate=False)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "            ('selector', get_numeric_data)\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', LinearSVC(class_weight='balanced', random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X, y, model, kf):\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    dicts = []\n",
    "    \n",
    "    for train_index, test_index in tqdm(kf.split(X, y)):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train = pd.DataFrame(data=X_train[0:,0:],\n",
    "                               columns=[feature_columns])\n",
    "        X_test = pd.DataFrame(data=X_test[0:,0:],\n",
    "                               columns=[feature_columns])\n",
    "\n",
    "        X_train[feature_columns] = X_train[feature_columns].apply(pd.to_numeric)\n",
    "        X_test[feature_columns] = X_test[feature_columns].apply(pd.to_numeric)\n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        dicts.append(report)\n",
    "    \n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172ac162423244dc8f9331d0a9d2f077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "dicts = cross_val(X, y, model, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['best', 'worst', 'average', \n",
    "           'micro avg', 'macro avg', 'weighted avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.506</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.489</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.596</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score\n",
       "best              0.263   0.213     0.123\n",
       "worst             0.506   0.548     0.416\n",
       "average           0.699   0.498     0.477\n",
       "micro avg         0.480   0.480     0.480\n",
       "macro avg         0.489   0.420     0.339\n",
       "weighted avg      0.596   0.480     0.420"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\n",
    "    'precision': [],\n",
    "    'recall': [], \n",
    "    'f1-score': []\n",
    "}\n",
    "\n",
    "for cl in classes:\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    support = []\n",
    "    for d in dicts:\n",
    "        precision.append(d[cl]['precision'])\n",
    "        recall.append(d[cl]['recall'])\n",
    "        f1_score.append(d[cl]['f1-score'])\n",
    "    result['precision'].append(round(statistics.mean(precision), 3))\n",
    "    result['recall'].append(round(statistics.mean(recall), 3))\n",
    "    result['f1-score'].append(round(statistics.mean(f1_score),3))\n",
    "result = pd.DataFrame(data=result, index=classes)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BorutaPy(alpha=0.05,\n",
       "     estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=5, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=187, n_jobs=-1, oob_score=False,\n",
       "            random_state=<mtrand.RandomState object at 0x10584e048>,\n",
       "            verbose=0, warm_start=False),\n",
       "     max_iter=100, n_estimators='auto', perc=100,\n",
       "     random_state=<mtrand.RandomState object at 0x10584e048>,\n",
       "     two_step=True, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=SEED)\n",
    "feat_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdvV\n",
      "CTTR\n",
      "CVVI\n",
      "D\n",
      "LFP_first_procent\n",
      "LFP_none\n",
      "LFP_third_procent\n",
      "LS\n",
      "LV\n",
      "LogTTR\n",
      "MCI\n",
      "ModV\n",
      "NDW\n",
      "NV\n",
      "RTTR\n",
      "SVVI\n",
      "TTR\n",
      "VVII\n",
      "acl\n",
      "advcl\n",
      "av_depth\n",
      "count_sent\n",
      "count_tokens\n",
      "density\n",
      "der_suff3\n",
      "der_suff4\n",
      "der_suff6\n",
      "freq_finite_forms\n",
      "gerund_tokens\n",
      "infinitive_tokens\n",
      "link_Clarification\n",
      "link_Contrast\n",
      "link_Sentence connectors(i.e.)\n",
      "link_Subordinators\n",
      "link_Subordinators(while)\n",
      "link_all\n",
      "max_depth\n",
      "mean_len_sent\n",
      "num_4grams\n",
      "parts\n",
      "pres_sg_tokens\n",
      "tokens_before_root\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(feat_selector.support_):\n",
    "    if feature == True:\n",
    "        print(feature_columns[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "REALEC_Inspector",
   "language": "python",
   "name": "realec_inspector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
