{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import ParserUDpipe\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(\"They go.\")\n",
    "df = parser.conllu2df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Form</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>UPosTag</th>\n",
       "      <th>XPosTag</th>\n",
       "      <th>Feats</th>\n",
       "      <th>Head</th>\n",
       "      <th>DepRel</th>\n",
       "      <th>Deps</th>\n",
       "      <th>Misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>They</td>\n",
       "      <td>they</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PE</td>\n",
       "      <td>Number=Plur|Person=3|PronType=Prs</td>\n",
       "      <td>2</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>go</td>\n",
       "      <td>VERB</td>\n",
       "      <td>V</td>\n",
       "      <td>Mood=Ind|Number=Plur|Tense=Pres|VerbForm=Fin</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>_</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>FS</td>\n",
       "      <td>_</td>\n",
       "      <td>2</td>\n",
       "      <td>punct</td>\n",
       "      <td>_</td>\n",
       "      <td>SpaceAfter=No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Form Lemma UPosTag XPosTag  \\\n",
       "0   1  They  they    PRON      PE   \n",
       "1   2    go    go    VERB       V   \n",
       "2   3     .     .   PUNCT      FS   \n",
       "\n",
       "                                          Feats  Head DepRel Deps  \\\n",
       "0             Number=Plur|Person=3|PronType=Prs     2  nsubj    _   \n",
       "1  Mood=Ind|Number=Plur|Tense=Pres|VerbForm=Fin     0   root    _   \n",
       "2                                             _     2  punct    _   \n",
       "\n",
       "            Misc  \n",
       "0              _  \n",
       "1  SpaceAfter=No  \n",
       "2  SpaceAfter=No  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dolphins love little dolphins.\n",
    "forms = 3: 'love', 'dolphins', 'little'\n",
    "lemmas = 3: 'love', 'dolphin', 'little'\n",
    "tokens = 4: 'dolphins', 'love', 'dolphins', 'little'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверить на punct\n",
    "# сначала спеллчеккер\n",
    "# разобраться с D\n",
    "# https://github.com/kristopherkyle/lexical_diversity отсюда взять критерии\n",
    "# нужно бы чтобы токены и лексемы были все в нижнем регистре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "\n",
    "open_class = [\"NOUN\", \"VERB\", \"ADV\", \"ADJ\"]\n",
    "with open('lists.json') as data_file:\n",
    "    lists = json.load(data_file)\n",
    "fivetfrequentCOCA = lists['5000frequentCOCA']\n",
    "frequentverbsCOCAfromfivet = lists['frequentverbsCOCAfrom5000']\n",
    "uwl = lists['UWL']\n",
    "\n",
    "class LexicalComplexity:\n",
    "    \"\"\"Returns values of lexical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    \n",
    "    def get_verb_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'VERB']['Lemma']\n",
    "    \n",
    "    def get_noun_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'NOUN']['Lemma']\n",
    "\n",
    "    def get_adj_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADJ']['Lemma']\n",
    "    \n",
    "    def get_adv_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADV']['Lemma']\n",
    "    \n",
    "    def get_lex_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'].isin(open_class)]['Lemma']\n",
    "    \n",
    "    def get_lemmas(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Lemma']\n",
    "    \n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "    \n",
    "    def safe_divide(self, numerator, denominator):\n",
    "        if denominator == 0 or denominator == 0.0:\n",
    "            index = 0\n",
    "        else: index = numerator/denominator\n",
    "        return index\n",
    "\n",
    "    def division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def corrected_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(2*len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def root_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def squared_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)**2/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def log_division(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))/math.log(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def uber(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))**2/math.log(len(set(list2))/len(list1))\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def density(self, punct=False):\n",
    "        \"\"\"\n",
    "        number of lexical tokens/number of tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        lemmas = self.get_lemmas()\n",
    "        return self.division(lex_lemmas, lemmas)\n",
    "    \n",
    "    def LS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated lexical tokens/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        soph_lex_lemmas = [i for i in lex_lemmas if i not in fivetfrequentCOCA]\n",
    "        return self.division(soph_lex_lemmas, lex_lemmas)\n",
    "    \n",
    "    def VS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated verb lemmas/number of verb tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = self.get_verb_lemmas()\n",
    "        soph_verbs = set([i for i in verb_lemmas if i not in frequentverbsCOCAfromfivet])\n",
    "        VSI = self.division(soph_verbs, verb_lemmas)\n",
    "        VSII = self.corrected_division(soph_verbs, verb_lemmas)\n",
    "        VSIII = self.squared_division(soph_verbs, verb_lemmas)\n",
    "        return VSI, VSII, VSIII\n",
    "\n",
    "    def LFP(self):\n",
    "        \"\"\"\n",
    "        Lexical Frequency Profile is the proportion of tokens:\n",
    "        first - 1000 most frequent words\n",
    "        second list - the second 1000\n",
    "        third - University Word List (Xue & Nation 1989)\n",
    "        none - list of those that are not in these lists\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        first = [i for i in lemmas if i in fivetfrequentCOCA[0:1000]]\n",
    "        second = [i for i in lemmas if i in fivetfrequentCOCA[1000:2000]]\n",
    "        third = [i for i in lemmas if i in uwl]\n",
    "        first_procent = self.division(first, lemmas)\n",
    "        second_procent = self.division(second, lemmas)\n",
    "        third_procent = self.division(third, lemmas)\n",
    "        none = 1 - (first_procent + second_procent + third_procent)\n",
    "        return first_procent, second_procent , third_procent, none\n",
    "    \n",
    "    def NDW(self):\n",
    "        \"\"\"\n",
    "        number of lemmas\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        return len(set(lemmas))\n",
    "    \n",
    "    def TTR(self):\n",
    "        \"\"\"\n",
    "        number of lemmas/number of tokens\n",
    "        \"\"\"\n",
    "        lemmas = set(self.get_lemmas())\n",
    "        tokens = self.get_lemmas()\n",
    "        TTR = self.division(lemmas, tokens)\n",
    "        CTTR = self.corrected_division(lemmas, tokens)\n",
    "        RTTR = self.root_division(lemmas, tokens)\n",
    "        LogTTR = self.log_division(lemmas, tokens)\n",
    "        Uber = self.uber(lemmas, tokens)\n",
    "        return TTR, CTTR, RTTR, LogTTR, Uber\n",
    "\n",
    "    def choose(self, n, k):\n",
    "        \"\"\"\n",
    "        Calculates binomial coefficients\n",
    "        \"\"\"\n",
    "        if 0 <= k <= n:\n",
    "            ntok = 1\n",
    "            ktok = 1\n",
    "            for t in range(1, min(k, n - k) + 1):\n",
    "                ntok *= n\n",
    "                ktok *= t\n",
    "                n -= 1\n",
    "            return ntok // ktok\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def hyper(self, successes, sample_size, population_size, freq):\n",
    "        \"\"\"\n",
    "        Calculates hypergeometric distribution\n",
    "        \"\"\"\n",
    "        # probability a word will occur at least once in a sample of a particular size\n",
    "        try:\n",
    "            prob_1 = 1.0 - (float((self.choose(freq, successes) * \n",
    "                                   self.choose((population_size - freq),\n",
    "                                               (sample_size - successes)))) /\n",
    "                            float(self.choose(population_size, sample_size)))\n",
    "            prob_1 = prob_1 * (1/sample_size)\n",
    "        except ZeroDivisionError:\n",
    "            prob_1 = 0\n",
    "        return prob_1\n",
    "    \n",
    "    def D(self):\n",
    "        prob_sum = 0.0\n",
    "        tokens = self.get_forms()\n",
    "        num_tokens = len(tokens)\n",
    "        types_list = list(set(tokens))\n",
    "        frequency_dict = collections.Counter(tokens)\n",
    "\n",
    "        for items in types_list:\n",
    "            # random sample is 42 items in length\n",
    "            prob = self.hyper(0, 42, num_tokens, frequency_dict[items])\n",
    "            prob_sum += prob\n",
    "\n",
    "        return prob_sum\n",
    "\n",
    "    def LV(self):\n",
    "        \"\"\"\n",
    "        number of lexical lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = set(self.get_lex_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return len(lex_lemmas)/len(lex_tokens)\n",
    "    \n",
    "    def VV(self):\n",
    "        \"\"\"\n",
    "        VVI: number of verb lemmas/number of verb tokens\n",
    "        VVII: number of verb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = set(self.get_verb_lemmas())\n",
    "        verb_tokens = self.get_verb_lemmas()\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        VVI = self.division(verb_lemmas, verb_tokens)\n",
    "        SVVI = self.squared_division(verb_lemmas, verb_tokens)\n",
    "        CVVI = self.corrected_division(verb_lemmas, verb_tokens)\n",
    "        VVII = self.division(verb_lemmas, lex_tokens)\n",
    "        return VVI, SVVI, CVVI, VVII\n",
    "        \n",
    "    def NV(self):\n",
    "        \"\"\"\n",
    "        number of noun lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        noun_lemmas = set(self.get_noun_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(noun_lemmas, lex_tokens)\n",
    "\n",
    "    def AdjV(self):\n",
    "        \"\"\"\n",
    "        number of adjective lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adj_lemmas = set(self.get_adj_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(adj_lemmas, lex_tokens)\n",
    "    \n",
    "    def AdvV(self):\n",
    "        \"\"\"\n",
    "        number of adverb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adv_lemmas = set(self.get_adv_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(adv_lemmas, lex_tokens)\n",
    "    \n",
    "    def ModV(self):\n",
    "        return self.AdjV() + self.AdvV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_criteria.csv', 'r') as file:\n",
    "    f = file.read()\n",
    "paths = ['/Users/irene/Desktop/Диплом/new_data/'+x+'.txt' for x in f.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "import enchant.checker\n",
    "from enchant.checker.CmdLineChecker import CmdLineChecker\n",
    "def check_spelling(text):\n",
    "    chkr = enchant.checker.SpellChecker(\"en_GB\")\n",
    "    chkr.set_text(text)\n",
    "    for err in chkr:\n",
    "        sug = err.suggest()[0]\n",
    "        err.replace(sug)\n",
    "    c = chkr.get_text()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The graph contains information about money people spend on petrol. The research was done in the USA and the UK. Three classes were compared: the poorest, the richest and middle-income people.\\nResults in two countries are absolutely the opposite. The UK-line gradually goes up, and reaches the peak on the point of 4 per cent. While the USA-line declines from the point of 5,3 per cent to 2,2 per cent. It means that the biggest amount of money is spent in the USA by poorest people. The same class in the UK spends only 0,5 per cent of the income. The difference in part of rich people is modest - about 1 per cent.\\nOverall, people from the USA spend bigger part of their income on petrol.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/irene/Desktop/Диплом/new_data/1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "LC = LexicalComplexity(text)\n",
    "dict_lex_comp = {'density': LC.density(), 'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "                 'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "                 'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "                 'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "                 'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.D(), \n",
    "                 'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "                 'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "                 'ModV': LC.ModV()}\n",
    "lex_criteria = list(dict_lex_comp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths[191:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lex_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050fd21ad8594d53ab82cabdb31e8582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r = ''\n",
    "for path in tqdm(paths):\n",
    "    string = path + ','\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "    text = check_spelling(text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    parser = ParserUDpipe(text)\n",
    "    LC = LexicalComplexity(text)\n",
    "    dict_lex_comp = {'density': LC.density(), 'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "                     'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "                     'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "                     'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "                     'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.D(), \n",
    "                     'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "                     'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "                     'ModV': LC.ModV()}\n",
    "    for c in lex_criteria:\n",
    "        string += str(round(dict_lex_comp[c], 5)) + ','\n",
    "    string += '\\n'\n",
    "    r += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('lex_criteria.csv', 'w') as file:\n",
    "    file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['density',\n",
       " 'LS',\n",
       " 'VSI',\n",
       " 'VSII',\n",
       " 'VSIII',\n",
       " 'LFP_first',\n",
       " 'LFP_second',\n",
       " 'LFP_third',\n",
       " 'LFP_none',\n",
       " 'NDW',\n",
       " 'TTR',\n",
       " 'CTTR',\n",
       " 'RTTR',\n",
       " 'LogTTR',\n",
       " 'Uber',\n",
       " 'D',\n",
       " 'LV',\n",
       " 'VVI',\n",
       " 'SVVI',\n",
       " 'CVVI',\n",
       " 'VVII',\n",
       " 'NV',\n",
       " 'AdjV',\n",
       " 'AdvV',\n",
       " 'ModV']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "LC = LexicalComplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lex_comp = {'density': LC.density(), 'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "                 'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "                 'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "                 'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "                 'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.D(), \n",
    "                 'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "                 'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "                 'ModV': LC.ModV()}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AdjV': 0.21739130434782608,\n",
       " 'AdvV': 0.043478260869565216,\n",
       " 'CTTR': 4.571428571428571,\n",
       " 'CVVI': 2.456769074559977,\n",
       " 'D': 0.8416985044788518,\n",
       " 'LFP_first': 0.8163265306122449,\n",
       " 'LFP_none': 0.04081632653061229,\n",
       " 'LFP_second': 0.09183673469387756,\n",
       " 'LFP_third': 0.05102040816326531,\n",
       " 'LS': 0.043478260869565216,\n",
       " 'LV': 0.8695652173913043,\n",
       " 'LogTTR': 0.9070692655306586,\n",
       " 'ModV': 0.2608695652173913,\n",
       " 'NDW': 64,\n",
       " 'NV': 0.34782608695652173,\n",
       " 'RTTR': 6.464976285134148,\n",
       " 'SVVI': 12.071428571428571,\n",
       " 'TTR': 0.6530612244897959,\n",
       " 'Uber': 0,\n",
       " 'VSI': 0.0,\n",
       " 'VSII': 0.0,\n",
       " 'VSIII': 0.0,\n",
       " 'VVI': 0.9285714285714286,\n",
       " 'VVII': 0.2826086956521739,\n",
       " 'density': 0.46938775510204084}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_lex_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# лучше выделять суффиксы\n",
    "# научиться выделять приставки\n",
    "# MCI: went -> ed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "go\t\tinf\t\t\t\t\t\tVerbForm=Inf\n",
    "going\tgerund\t\t\t\t\tVerbForm=Ger\n",
    "goes\t3rd singular present\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
    "go\t\tpresent\t\t\t\t\tMood=Ind|Number=Plur|Tense=Pres|VerbForm=Fin\n",
    "gone\tpast participle\t\t\tTense=Past|VerbForm=Part\n",
    "went\tsimple past\t\t\t\tMood=Ind|Person=3|Tense=Past|VerbForm=Fin\n",
    "\n",
    "df[df['Feats'].str.match('.*VerbForm=Inf.*')]\n",
    "df[df['Feats'].str.match('.*VerbForm=Ger.*')]\n",
    "df[df['Feats'].str.match('.*Mood=Ind.+Number=Sing.+Person=3.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "df[df['Feats'].str.match('.*Mood=Ind.+Number=Plur.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "df[df['Feats'].str.match('.*Tense=Past.+VerbForm=Part.*')]\n",
    "df[df['Feats'].str.match('.*Mood=Ind.+Person=3.+Tense=Past.+VerbForm=Fin.*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "with open('suffixes.json') as data_file:\n",
    "    suffixes_levels = json.load(data_file)\n",
    "\n",
    "class MorphologicalComplexity:\n",
    "    \"\"\"Returns values of morphological criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "   \n",
    "    def get_inf(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*VerbForm=Inf.*')]\n",
    "    \n",
    "    def get_gerund(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*VerbForm=Ger.*')]\n",
    "    \n",
    "    def get_pres_sg(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Mood=Ind.+Number=Sing.+Person=3.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "    \n",
    "    def get_pres_pl(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Mood=Ind.+Number=Plur.+Tense=Pres.+VerbForm=Fin.*')]\n",
    "    \n",
    "    def get_part(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Tense=Past.+VerbForm=Part.*')]\n",
    "    \n",
    "    def get_past(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.match('.*Mood=Ind.+Person=3.+Tense=Past.+VerbForm=Fin.*')]     \n",
    "        \n",
    "    def get_verb_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'VERB']['Form']\n",
    "    \n",
    "    def get_verb_feats(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['Feats'].str.contains('VerbForm=')]['Feats']\n",
    "    \n",
    "    def get_aux(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'AUX']['Form']\n",
    "    \n",
    "    def division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def one_random_list(self, l, length):\n",
    "        result = []\n",
    "        for i in range(length):\n",
    "            random_element = random.choice(l)\n",
    "            l.remove(random_element)\n",
    "            result.append(random_element)\n",
    "        return result, l\n",
    "\n",
    "    def two_random_lists(self, l, length=10):\n",
    "        list1, list2 = [], []\n",
    "        if len(l) < length*2:\n",
    "            return self.two_random_lists(l, length=length-1)\n",
    "        else:\n",
    "            list1, l = self.one_random_list(l, length)\n",
    "            list2, l = self.one_random_list(l, length)\n",
    "            return list1, list2\n",
    "        \n",
    "    def num_uniques(self, l):\n",
    "        counter=collections.Counter(l)\n",
    "        return list(counter.values()).count(1)\n",
    "\n",
    "    def get_suffix(self, word):\n",
    "        root = porter_stemmer.stem(word)\n",
    "        suffix = word[len(root):]\n",
    "        return suffix\n",
    "    \n",
    "    def get_suffixes(self):\n",
    "        forms = self.get_forms()\n",
    "        suffixes = [self.get_suffix(word) for word in forms]\n",
    "        return list(filter(lambda s: s != \"\", suffixes))\n",
    "        \n",
    "    def derivational_suffixation(self):\n",
    "        \"\"\"\n",
    "        number of suffixes on n's level/number of suffixes\n",
    "        \"\"\"\n",
    "        suffixes = self.get_suffixes()\n",
    "        level3_suffixes = [i for i in suffixes if i in suffixes_levels[\"level3\"]]\n",
    "        level4_suffixes = [i for i in suffixes if i in suffixes_levels[\"level4\"]]\n",
    "        level5_suffixes = [i for i in suffixes if i in suffixes_levels[\"level5\"]]\n",
    "        level6_suffixes = [i for i in suffixes if i in suffixes_levels[\"level6\"]]\n",
    "        der_suff3 = self.division(level3_suffixes, suffixes)\n",
    "        der_suff4 = self.division(level4_suffixes, suffixes)\n",
    "        der_suff5 = self.division(level5_suffixes, suffixes)\n",
    "        der_suff6 = self.division(level6_suffixes, suffixes)\n",
    "        return der_suff3, der_suff4, der_suff5, der_suff6\n",
    "    \n",
    "    def MCI(self):\n",
    "        \"\"\"\n",
    "        MCI represents the average inflectional diversity for the parts of speech in the sample\n",
    "        \"\"\"\n",
    "        verb_forms = self.get_verb_forms()\n",
    "        suff_verb = [self.get_suffix(verb) for verb in verb_forms]\n",
    "        list1, list2 = self.two_random_lists(suff_verb)\n",
    "        diversity1=len(set(list1))\n",
    "        diversity2=len(set(list2))\n",
    "        mean_diversity = (diversity1+diversity2)/2\n",
    "        num_uni = self.num_uniques(list1+list2)\n",
    "        IUV = num_uni/2\n",
    "        MCI = mean_diversity + IUV/2 - 1\n",
    "        return MCI\n",
    "    \n",
    "    def freq_finite_forms(self):\n",
    "        \"\"\"\n",
    "        frequency of tensed(finite) forms\n",
    "        \"\"\"\n",
    "        verb_feats = self.get_verb_feats()\n",
    "        finite_forms = [word for word in verb_feats if \"VerbForm=Fin\" in word]\n",
    "        return self.division(finite_forms, verb_feats)\n",
    "    \n",
    "    def freq_aux(self):\n",
    "        \"\"\"\n",
    "        frequency of modals(auxilaries)\n",
    "        \"\"\"\n",
    "        verb_feats = self.get_verb_feats()\n",
    "        aux = self.get_aux()\n",
    "        return self.division(aux, verb_feats)\n",
    "\n",
    "    def num_verb_forms(self):\n",
    "        \"\"\"\n",
    "        number of different verb forms: \n",
    "        infinitives, gerunds, present singular, present plural, past participle, past simple\n",
    "        \"\"\"\n",
    "        inf = self.get_inf()\n",
    "        gerund = self.get_gerund()\n",
    "        pres_sg = self.get_pres_sg()\n",
    "        pres_pl = self.get_pres_pl()\n",
    "        part = self.get_part()\n",
    "        past = self.get_past()\n",
    "        return len(inf), len(gerund), len(pres_sg), len(pres_pl), len(part), len(past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "MC = MorphologicalComplexity(text)\n",
    "dict_morph_comp = {'der_suff_level3': MC.derivational_suffixation()[0], \n",
    "                   'der_suff_level4': MC.derivational_suffixation()[1], \n",
    "                   'der_suff_level5': MC.derivational_suffixation()[2], \n",
    "                   'der_suff_level6': MC.derivational_suffixation()[3],\n",
    "                   'MCI': MC.MCI(), 'freq_finite_forms': MC.freq_finite_forms(), \n",
    "                   'freq_aux': MC.freq_aux(), 'num_inf': MC.num_verb_forms()[0], \n",
    "                   'num_inf': MC.num_verb_forms()[0], 'num_gerund': MC.num_verb_forms()[1], \n",
    "                   'num_pres_sg': MC.num_verb_forms()[2], 'num_pres_pl': MC.num_verb_forms()[3], \n",
    "                   'num_part': MC.num_verb_forms()[4], 'num_past': MC.num_verb_forms()[5]} \n",
    "morph_criteria = list(dict_morph_comp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86253d7fa1a49bba6f75ea6c31287b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r = ''\n",
    "for path in tqdm(paths):\n",
    "    string = path + ','\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "    text = check_spelling(text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    parser = ParserUDpipe(text)\n",
    "    MC = MorphologicalComplexity(text)\n",
    "    dict_morph_comp = {'der_suff_level3': MC.derivational_suffixation()[0], \n",
    "                       'der_suff_level4': MC.derivational_suffixation()[1], \n",
    "                       'der_suff_level5': MC.derivational_suffixation()[2], \n",
    "                       'der_suff_level6': MC.derivational_suffixation()[3],\n",
    "                       'MCI': MC.MCI(), 'freq_finite_forms': MC.freq_finite_forms(), \n",
    "                       'freq_aux': MC.freq_aux(), 'num_inf': MC.num_verb_forms()[0], \n",
    "                       'num_inf': MC.num_verb_forms()[0], 'num_gerund': MC.num_verb_forms()[1], \n",
    "                       'num_pres_sg': MC.num_verb_forms()[2], 'num_pres_pl': MC.num_verb_forms()[3], \n",
    "                       'num_part': MC.num_verb_forms()[4], 'num_past': MC.num_verb_forms()[5]}  \n",
    "    for c in morph_criteria:\n",
    "        string += str(round(dict_morph_comp[c], 5)) + ','\n",
    "    string += '\\n'\n",
    "    r += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('morph_criteria.csv', 'w') as file:\n",
    "    file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['der_suff_level3',\n",
       " 'der_suff_level4',\n",
       " 'der_suff_level5',\n",
       " 'der_suff_level6',\n",
       " 'MCI',\n",
       " 'freq_finite_forms',\n",
       " 'freq_aux',\n",
       " 'num_inf',\n",
       " 'num_gerund',\n",
       " 'num_pres_sg',\n",
       " 'num_pres_pl',\n",
       " 'num_part',\n",
       " 'num_past']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "MC = MorphologicalComplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_morph_comp = {'der_suff_level3': MC.derivational_suffixation()[0], \n",
    "                   'der_suff_level4': MC.derivational_suffixation()[1], \n",
    "                   'der_suff_level5': MC.derivational_suffixation()[2], \n",
    "                   'der_suff_level6': MC.derivational_suffixation()[3],\n",
    "                   'MCI': MC.MCI(), 'freq_finite_forms': MC.freq_finite_forms(), \n",
    "                   'freq_aux': MC.freq_aux(), 'num_inf': MC.num_verb_forms()[0], \n",
    "                   'num_inf': MC.num_verb_forms()[0], 'num_gerund': MC.num_verb_forms()[1], \n",
    "                   'num_pres_sg': MC.num_verb_forms()[2], 'num_pres_pl': MC.num_verb_forms()[3], \n",
    "                   'num_part': MC.num_verb_forms()[4], 'num_past': MC.num_verb_forms()[5]}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MCI': 2.75,\n",
       " 'der_suff_level3': 0.038461538461538464,\n",
       " 'der_suff_level4': 0.038461538461538464,\n",
       " 'der_suff_level5': 0.07692307692307693,\n",
       " 'der_suff_level6': 0.0,\n",
       " 'freq_aux': 0.3333333333333333,\n",
       " 'freq_finite_forms': 0.8,\n",
       " 'num_gerund': 0,\n",
       " 'num_inf': 0,\n",
       " 'num_part': 3,\n",
       " 'num_past': 1,\n",
       " 'num_pres_pl': 2,\n",
       " 'num_pres_sg': 8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_morph_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of tokens, minimum/maximum/average depth of the sentence, number of relative clauses, number of adverbial clauses, number of modifier clauses, number of sentences, number of clauses, number of T-units, number of complex T-units, number of coordinate phrases, number of noun phrases (possessive structures,  prepositional phrases, infinitives or gerunds in the position of object or subject, phrases like “adjective + noun”, “participle + noun”, “noun + infinitive”), number of complex noun phrases, number of verb phrases, Coordination Index, variety of constructions, average number of tokens before the root of the sentence, mean length of the sentence, mean length of the clause, number of clauses per sentence, number of clauses per T-unit, number of dependent clauses per clause, number of dependent clauses per T-unit, number of coordinate phrases per clause, number of T-units per sentence, number of possessive structures per sentence,  number of prepositional phrases per sentence, number of infinitives or gerunds in the position of object or subject per sentence, number of phrases like “adjective + noun”, “participle + noun”, “noun + infinitive” per sentence,  number of verb phrases per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# дописать класс\n",
    "# дополнить критерием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class SyntacticComplexity:\n",
    "    \"\"\"Returns values of syntactical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.get_forms())\n",
    "\n",
    "    def order_head(self, sent):\n",
    "        _id = sent['Id']\n",
    "        _head = sent['Head']\n",
    "        _form = sent['Form']\n",
    "        return(list(zip(_id, _head)))\n",
    "    \n",
    "    def get_dep_rel(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['DepRel']\n",
    "\n",
    "    def find_root(self, order_head_lst):\n",
    "        for every_order_head in order_head_lst:\n",
    "            if every_order_head[1] == 0:\n",
    "                root = every_order_head\n",
    "        return root\n",
    "\n",
    "    def root_children(self, sent):\n",
    "        order_head_lst = self.order_head(sent)\n",
    "        root = self.find_root(order_head_lst)\n",
    "        chains = []\n",
    "        for every_order_head in order_head_lst:\n",
    "            if every_order_head[1] == root[0]:\n",
    "                chains.append([root[0], every_order_head[0]])\n",
    "        return chains, order_head_lst\n",
    "\n",
    "    def chains_heads(self, chains, order_head_lst):\n",
    "        length_chains = len(chains)\n",
    "        i = 0\n",
    "        for chain in chains:\n",
    "            if i < length_chains:\n",
    "                heads = []\n",
    "                if 'stop' not in chain:\n",
    "                    for order_head in order_head_lst:\n",
    "                        if chain[-1] == order_head[1]:\n",
    "                            heads.append(order_head[0])\n",
    "                    if heads == [] and 'stop' not in chain:\n",
    "                        chain.append('stop')\n",
    "                    else:\n",
    "                        ind_head = 0\n",
    "                        for head in heads:\n",
    "                            new_chain = copy.copy(chain)[:-1]\n",
    "                            if ind_head == 0:\n",
    "                                chain.append(head)\n",
    "                                ind_head += 1\n",
    "                            else:\n",
    "                                new_chain.append(head)\n",
    "                                chains.append(new_chain)\n",
    "            i += 1\n",
    "        while all(item[-1] == 'stop' for item in chains) is False:\n",
    "            self.chains_heads(chains, order_head_lst)\n",
    "        return chains\n",
    "\n",
    "    def count_depth_for_one_sent(self, sent):\n",
    "        chains, order_head_lst = self.root_children(sent)\n",
    "        chains = self.chains_heads(chains, order_head_lst)\n",
    "        depths = []\n",
    "        for chain in chains:\n",
    "            depths.append(len(chain)-2)\n",
    "        return max(depths)\n",
    "\n",
    "    def count_depths(self):\n",
    "        max_depths = []\n",
    "        sentances, df_sentences = parser.conllu2df(sentences=True)\n",
    "        for sent in df_sentences:\n",
    "            max_depths.append(self.count_depth_for_one_sent(sent))\n",
    "        return max_depths\n",
    "    \n",
    "    def av_depth(self):\n",
    "        max_depths = self.count_depths()\n",
    "        return round(np.mean(max_depths), 2)\n",
    "    \n",
    "    def max_depth(self):\n",
    "        max_depths = self.count_depths()\n",
    "        return round(np.max(max_depths),2)\n",
    "    \n",
    "    def min_depth(self):\n",
    "        max_depths = self.count_depths()\n",
    "        return round(np.min(max_depths), 2)\n",
    "    \n",
    "    def find_in_dict(self, d, v):\n",
    "        try:\n",
    "            return d[v]\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def count_dep_sent(self):\n",
    "        dep_rel = self.get_dep_rel()           \n",
    "        dict_dep_rel = collections.Counter(dep_rel)\n",
    "        acl = self.find_in_dict(dict_dep_rel, 'acl')\n",
    "        rel_cl = self.find_in_dict(dict_dep_rel, 'acl:relcl')\n",
    "        advcl = self.find_in_dict(dict_dep_rel, 'advcl')\n",
    "        return acl, rel_cl, advcl\n",
    "    \n",
    "    def count_sent(parsed_text):\n",
    "        sentances, df_sentences = parser.conllu2df(sentences=True)\n",
    "        return len(sentances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The graph contains information about money people spend on petrol. The research was done in the USA and the UK. Three classes were compared: the poorest, the richest and middle-income people.\\nResults in two countries are absolutely the opposite. The UK-line gradually goes up, and reaches the peak on the point of 4 per cent. While the USA-line declines from the point of 5,3 per cent to 2,2 per cent. It means that the biggest amount of money is spent in the USA by poorest people. The same class in the UK spends only 0,5 per cent of the income. The difference in part of rich people is modest - about 1 per cent.\\nOverall, people from the USA spend bigger part of their income on petrol.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parsing import ParserUDpipe\n",
    "with open('/Users/irene/Desktop/Диплом/new_data/1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "text\n",
    "#parser = ParserUDpipe(text)\n",
    "#SC = SyntacticComplexity(text)\n",
    "#SC.count_dep_sent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhetorical Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# научиться выделять discourse-organising nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('linkings.json') as data_file:\n",
    "    linkings = json.load(data_file)\n",
    "\n",
    "with open('ngrams.txt') as data_file:\n",
    "    ngrams = [x.split() for x in data_file.read().split('\\n')]\n",
    "    \n",
    "with open('functional_ngrams.json') as data_file:\n",
    "    func_ngrams = json.load(data_file)\n",
    "    \n",
    "import re\n",
    "\n",
    "    \n",
    "class RhetoricalComplexity:\n",
    "    \"\"\"Returns values of rhetorical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        \n",
    "    def get_forms(self):\n",
    "        df = parser.conllu2df()\n",
    "        return df['Form']\n",
    "\n",
    "    def subfinder(self, mylist, pattern):\n",
    "        matches = []\n",
    "        for i in range(len(mylist)):\n",
    "            if mylist[i] == pattern[0] and mylist[i:i+len(pattern)] == pattern:\n",
    "                matches.append(pattern)\n",
    "        return matches\n",
    "    \n",
    "    def num_dict_2_levels(self, d, prefix):\n",
    "        num_all = 0\n",
    "        result = {}\n",
    "        for group in d:\n",
    "            num_group = 0\n",
    "            #print(d[group])\n",
    "            for subgroup in d[group]:\n",
    "                num_subgroup = 0\n",
    "                name_subgroup = list(subgroup.keys())[0]\n",
    "                for word in list(subgroup.values())[0]:\n",
    "                    num = len(re.findall(word.lower(), self.text.lower()))\n",
    "                    num_all += num\n",
    "                    num_subgroup += num\n",
    "                    num_group += num\n",
    "                    result[prefix+name_subgroup+\"(\"+word+\")\"] = num\n",
    "                result[prefix+name_subgroup] = num_subgroup\n",
    "            result[prefix+group] = num_group\n",
    "        result[prefix+'all'] = num_all\n",
    "        return result\n",
    "    \n",
    "    def num_linkings(self):\n",
    "        \"\"\"\n",
    "        number of linking phrases (Swales & Feak 2009)\n",
    "        \"\"\"\n",
    "        num_links_d = self.num_dict_2_levels(linkings, 'link_')\n",
    "        return num_links_d\n",
    "    \n",
    "    def num_4grams(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        num_all = 0\n",
    "        for ngram in ngrams:\n",
    "            num = len(self.subfinder([x.lower() if type(x) == str else x for x in self.get_forms()], ngram))\n",
    "            num_all += num\n",
    "        return num_all\n",
    "    \n",
    "    def num_func_ngrams(self):\n",
    "        \"\"\"\n",
    "        number of linking phrases (Swales & Feak 2009)\n",
    "        \"\"\"\n",
    "        num_links_d = self.num_dict_2_levels(func_ngrams, '4grams_')\n",
    "        return num_links_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ParserUDpipe(text)\n",
    "RC = RhetoricalComplexity(text)\n",
    "_func_ngrams = RC.num_func_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grams = {'num_4grams': RC.num_4grams()}\n",
    "_func_ngrams = RC.num_func_ngrams()\n",
    "num_linkings = RC.num_linkings()\n",
    "_dict_rhet_comp = {**num_linkings, **_func_ngrams}\n",
    "dict_rhet_comp = {**_dict_rhet_comp, **num_grams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rhet_criteria = list(dict_rhet_comp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6bb7f4db894c999f85ea820e5f1e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r = ''\n",
    "for path in tqdm(paths):\n",
    "    string = path + ','\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "    text = check_spelling(text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    parser = ParserUDpipe(text)\n",
    "    RC = RhetoricalComplexity(text)\n",
    "    num_grams = {'num_4grams': RC.num_4grams()}\n",
    "    _func_ngrams = RC.num_func_ngrams()\n",
    "    num_linkings = RC.num_linkings()\n",
    "    _dict_rhet_comp = {**num_linkings, **_func_ngrams}\n",
    "    dict_rhet_comp = {**_dict_rhet_comp, **num_grams} \n",
    "    for c in rhet_criteria:\n",
    "        string += str(dict_rhet_comp[c]) + ','\n",
    "    string += '\\n'\n",
    "    r += string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/irene/Desktop/Диплом/new_data/259.txt,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,5,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rhet_criteria.csv', 'w') as file:\n",
    "    file.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['link_Sentence connectors(Furthermore)',\n",
       " 'link_Sentence connectors(In addition)',\n",
       " 'link_Sentence connectors(Moreover)',\n",
       " 'link_Sentence connectors',\n",
       " 'link_Phrases linkers(In addition)',\n",
       " 'link_Phrases linkers',\n",
       " 'link_Addition',\n",
       " 'link_subordinators(Although)',\n",
       " 'link_subordinators(Even though)',\n",
       " 'link_subordinators',\n",
       " 'link_Sentence connectors(However)',\n",
       " 'link_Sentence connectors(Nevertheless)',\n",
       " 'link_Phrase linkers(despite)',\n",
       " 'link_Phrase linkers(In spite of)',\n",
       " 'link_Phrase linkers',\n",
       " 'link_Adversativity',\n",
       " 'link_subordinators(Because)',\n",
       " 'link_subordinators(since)',\n",
       " 'link_Sentence connectors(Therefore)',\n",
       " 'link_Sentence connectors(As a result)',\n",
       " 'link_Sentence connectors(consequently)',\n",
       " 'link_Sentence connectors(Hence)',\n",
       " 'link_Sentence connectors(Thus)',\n",
       " 'link_Phrase linkers(Because of)',\n",
       " 'link_Phrase linkers(Due to)',\n",
       " 'link_Phrase linkers(As a result of)',\n",
       " 'link_Cause and effect',\n",
       " 'link_Sentence connectors(In other words)',\n",
       " 'link_Sentence connectors(That is)',\n",
       " 'link_Sentence connectors(i.e.)',\n",
       " 'link_Clarification',\n",
       " 'link_Subordinators(while)',\n",
       " 'link_Subordinators(whereas)',\n",
       " 'link_Subordinators',\n",
       " 'link_Sentence connectors(In contrast)',\n",
       " 'link_Sentence connectors(On the other hand)',\n",
       " 'link_Sentence connectors(Conversely)',\n",
       " 'link_Phrase linkers(Unlike)',\n",
       " 'link_Contrast',\n",
       " 'link_Sentence connectors(For example)',\n",
       " 'link_Sentence connectors(For instance)',\n",
       " 'link_Illustration',\n",
       " 'link_Sentence connectors(On the contrary)',\n",
       " 'link_Sentence connectors(As a matter of fact)',\n",
       " 'link_Sentence connectors(In fact)',\n",
       " 'link_Intensification',\n",
       " 'link_all',\n",
       " '4grams_Topic elaboration/clarification(and to be a)',\n",
       " '4grams_Topic elaboration/clarification(are more and more)',\n",
       " '4grams_Topic elaboration/clarification(as well as the)',\n",
       " '4grams_Topic elaboration/clarification(but there are still)',\n",
       " '4grams_Topic elaboration/clarification(can be divided into)',\n",
       " '4grams_Topic elaboration/clarification(how to deal with)',\n",
       " '4grams_Topic elaboration/clarification(if you don’t know)',\n",
       " '4grams_Topic elaboration/clarification(if you want to)',\n",
       " '4grams_Topic elaboration/clarification(in order to make)',\n",
       " '4grams_Topic elaboration/clarification(is a kind of)',\n",
       " '4grams_Topic elaboration/clarification(is based on the)',\n",
       " '4grams_Topic elaboration/clarification(is more important than)',\n",
       " '4grams_Topic elaboration/clarification(is totally different from)',\n",
       " '4grams_Topic elaboration/clarification(it is a good)',\n",
       " '4grams_Topic elaboration/clarification(it is a very)',\n",
       " '4grams_Topic elaboration/clarification(it is also a)',\n",
       " '4grams_Topic elaboration/clarification(it is because the)',\n",
       " '4grams_Topic elaboration/clarification(it is not a)',\n",
       " '4grams_Topic elaboration/clarification(on the other hand)',\n",
       " '4grams_Topic elaboration/clarification(there will be a)',\n",
       " '4grams_Topic elaboration/clarification(to cope with the)',\n",
       " '4grams_Topic elaboration/clarification(want to be a)',\n",
       " '4grams_Topic elaboration/clarification',\n",
       " '4grams_Identification/focus(my point of view)',\n",
       " '4grams_Identification/focus(the best way to)',\n",
       " '4grams_Identification/focus(a very important role)',\n",
       " '4grams_Identification/focus(as far as the)',\n",
       " '4grams_Identification/focus(as I have mentioned)',\n",
       " '4grams_Identification/focus(him or her to)',\n",
       " '4grams_Identification/focus(is one of my)',\n",
       " '4grams_Identification/focus(is one of the)',\n",
       " '4grams_Identification/focus(is the most important)',\n",
       " '4grams_Identification/focus(is very important for)',\n",
       " '4grams_Identification/focus(it is very important)',\n",
       " '4grams_Identification/focus(one of the most)',\n",
       " '4grams_Identification/focus(the most important thing)',\n",
       " '4grams_Identification/focus(we can say that)',\n",
       " '4grams_Identification/focus(we can see that)',\n",
       " '4grams_Identification/focus(we can see the)',\n",
       " '4grams_Identification/focus',\n",
       " '4grams_Topic introduction(I am going to)',\n",
       " '4grams_Topic introduction(I would like to)',\n",
       " '4grams_Topic introduction(if there is a)',\n",
       " '4grams_Topic introduction',\n",
       " '4grams_Discourse organizers',\n",
       " '4grams_Quantifying(a great deal of)',\n",
       " '4grams_Quantifying(a great number of)',\n",
       " '4grams_Quantifying(a large amount of)',\n",
       " '4grams_Quantifying(a lot of people)',\n",
       " '4grams_Quantifying(a lot of problem)',\n",
       " '4grams_Quantifying(a lot of problems)',\n",
       " '4grams_Quantifying(a lot of time)',\n",
       " '4grams_Quantifying(all of them are)',\n",
       " '4grams_Quantifying(and a lot of)',\n",
       " '4grams_Quantifying(bring a lot of)',\n",
       " '4grams_Quantifying(has a lot of)',\n",
       " '4grams_Quantifying(more and more people)',\n",
       " '4grams_Quantifying(most of the people)',\n",
       " '4grams_Quantifying(most of them are)',\n",
       " '4grams_Quantifying(some of them are)',\n",
       " '4grams_Quantifying(that it is more)',\n",
       " '4grams_Quantifying(the rest of the)',\n",
       " '4grams_Quantifying(the rest of the world)',\n",
       " '4grams_Quantifying(there are a lot of)',\n",
       " '4grams_Quantifying(there are many people)',\n",
       " '4grams_Quantifying(there are quite a)',\n",
       " '4grams_Quantifying(there are so many)',\n",
       " '4grams_Quantifying(there are still some)',\n",
       " '4grams_Quantifying(there are too many)',\n",
       " '4grams_Quantifying(with a lot of)',\n",
       " '4grams_Quantifying',\n",
       " '4grams_Time/place/text deixis(all over the world)',\n",
       " '4grams_Time/place/text deixis(at the beginning of the)',\n",
       " '4grams_Time/place/text deixis(at the same time)',\n",
       " '4grams_Time/place/text deixis(for a long time)',\n",
       " '4grams_Time/place/text deixis(in the following paragraphs)',\n",
       " '4grams_Time/place/text deixis(the end of the)',\n",
       " '4grams_Time/place/text deixis',\n",
       " '4grams_Framing(because they are not)',\n",
       " '4grams_Framing(in such a way)',\n",
       " '4grams_Framing(in the process of)',\n",
       " '4grams_Framing(on the basis of)',\n",
       " '4grams_Framing(the main reason is)',\n",
       " '4grams_Framing(the quality of the)',\n",
       " '4grams_Framing(the reason is that)',\n",
       " '4grams_Framing(the relationship between the)',\n",
       " '4grams_Framing(the result of the)',\n",
       " '4grams_Framing(with the development of)',\n",
       " '4grams_Framing(as a result of)',\n",
       " '4grams_Framing(as the result of)',\n",
       " '4grams_Framing(the result of this)',\n",
       " '4grams_Framing',\n",
       " '4grams_Referential',\n",
       " '4grams_Epistemic(as a matter of)',\n",
       " '4grams_Epistemic(as we all know)',\n",
       " '4grams_Epistemic(become more and more)',\n",
       " '4grams_Epistemic(I think it is)',\n",
       " '4grams_Epistemic(I think that this)',\n",
       " '4grams_Epistemic(I think the most)',\n",
       " '4grams_Epistemic(I think this is)',\n",
       " '4grams_Epistemic(it is believed that)',\n",
       " '4grams_Epistemic(it is obvious that)',\n",
       " '4grams_Epistemic(it is true that)',\n",
       " '4grams_Epistemic(some people think that)',\n",
       " '4grams_Epistemic',\n",
       " '4grams_Attitudinal/modality(are not allowed to)',\n",
       " '4grams_Attitudinal/modality(I hope I can)',\n",
       " '4grams_Attitudinal/modality(is very important to)',\n",
       " '4grams_Attitudinal/modality(it is difficult to)',\n",
       " '4grams_Attitudinal/modality(it is very difficult)',\n",
       " '4grams_Attitudinal/modality(it is hard to)',\n",
       " '4grams_Attitudinal/modality(it is not easy)',\n",
       " '4grams_Attitudinal/modality(necessary for us to)',\n",
       " '4grams_Attitudinal/modality(should learn how to)',\n",
       " '4grams_Attitudinal/modality(will not be able to)',\n",
       " '4grams_Attitudinal/modality',\n",
       " '4grams_Stance',\n",
       " '4grams_all',\n",
       " 'num_4grams']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhet_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and to be a\n",
      "are more and more\n",
      "as well as the\n",
      "but there are still\n",
      "can be divided into\n",
      "how to deal with\n",
      "if you don’t know\n",
      "if you want to\n",
      "in order to make\n",
      "is a kind of\n",
      "is based on the\n",
      "is more important than\n",
      "is totally different from\n",
      "it is a good\n",
      "it is a very\n",
      "it is also a\n",
      "it is because the\n",
      "it is not a\n",
      "on the other hand\n",
      "there will be a\n",
      "to cope with the\n",
      "want to be a\n",
      "my point of view\n",
      "the best way to\n",
      "a very important role\n",
      "as far as the\n",
      "as I have mentioned\n",
      "him or her to\n",
      "is one of my\n",
      "is one of the\n",
      "is the most important\n",
      "is very important for\n",
      "it is very important\n",
      "one of the most\n",
      "the most important thing\n",
      "we can say that\n",
      "we can see that\n",
      "we can see the\n",
      "I am going to\n",
      "I would like to\n",
      "if there is a\n",
      "a great deal of\n",
      "a great number of\n",
      "a large amount of\n",
      "a lot of people\n",
      "a lot of problem\n",
      "a lot of problems\n",
      "a lot of time\n",
      "all of them are\n",
      "and a lot of\n",
      "bring a lot of\n",
      "has a lot of\n",
      "has a lot of\n",
      "more and more people\n",
      "most of the people\n",
      "most of them are\n",
      "some of them are\n",
      "that it is more\n",
      "the rest of the\n",
      "the rest of the world\n",
      "there are a lot of\n",
      "there are many people\n",
      "there are quite a\n",
      "there are so many\n",
      "there are still some\n",
      "there are too many\n",
      "with a lot of\n",
      "all over the world\n",
      "at the beginning of the\n",
      "at the same time\n",
      "for a long time\n",
      "in the following paragraphs\n",
      "the end of the\n",
      "because they are not\n",
      "in such a way\n",
      "in the process of\n",
      "on the basis of\n",
      "the main reason is\n",
      "the quality of the\n",
      "the reason is that\n",
      "the relationship between the\n",
      "the result of the\n",
      "with the development of\n",
      "as a result of\n",
      "as the result of\n",
      "the result of this\n",
      "as a matter of\n",
      "as we all know\n",
      "become more and more\n",
      "I think it is\n",
      "I think that this\n",
      "I think the most\n",
      "I think this is\n",
      "it is believed that\n",
      "it is obvious that\n",
      "it is true that\n",
      "some people think that\n",
      "are not allowed to\n",
      "I hope I can\n",
      "is very important to\n",
      "it is difficult to\n",
      "it is very difficult\n",
      "it is hard to\n",
      "it is not easy\n",
      "necessary for us to\n",
      "should learn how to\n",
      "will not be able to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Attitudinal/modality': 0,\n",
       " 'Attitudinal/modality(I hope I can)': 0,\n",
       " 'Attitudinal/modality(are not allowed to)': 0,\n",
       " 'Attitudinal/modality(is very important to)': 0,\n",
       " 'Attitudinal/modality(it is difficult to)': 0,\n",
       " 'Attitudinal/modality(it is hard to)': 0,\n",
       " 'Attitudinal/modality(it is not easy)': 0,\n",
       " 'Attitudinal/modality(it is very difficult)': 0,\n",
       " 'Attitudinal/modality(necessary for us to)': 0,\n",
       " 'Attitudinal/modality(should learn how to)': 0,\n",
       " 'Attitudinal/modality(will not be able to)': 0,\n",
       " 'Discourse organizers': 0,\n",
       " 'Epistemic': 0,\n",
       " 'Epistemic(I think it is)': 0,\n",
       " 'Epistemic(I think that this)': 0,\n",
       " 'Epistemic(I think the most)': 0,\n",
       " 'Epistemic(I think this is)': 0,\n",
       " 'Epistemic(as a matter of)': 0,\n",
       " 'Epistemic(as we all know)': 0,\n",
       " 'Epistemic(become more and more)': 0,\n",
       " 'Epistemic(it is believed that)': 0,\n",
       " 'Epistemic(it is obvious that)': 0,\n",
       " 'Epistemic(it is true that)': 0,\n",
       " 'Epistemic(some people think that)': 0,\n",
       " 'Framing': 0,\n",
       " 'Framing(as a result of)': 0,\n",
       " 'Framing(as the result of)': 0,\n",
       " 'Framing(because they are not)': 0,\n",
       " 'Framing(in such a way)': 0,\n",
       " 'Framing(in the process of)': 0,\n",
       " 'Framing(on the basis of)': 0,\n",
       " 'Framing(the main reason is)': 0,\n",
       " 'Framing(the quality of the)': 0,\n",
       " 'Framing(the reason is that)': 0,\n",
       " 'Framing(the relationship between the)': 0,\n",
       " 'Framing(the result of the)': 0,\n",
       " 'Framing(the result of this)': 0,\n",
       " 'Framing(with the development of)': 0,\n",
       " 'Identification/focus': 0,\n",
       " 'Identification/focus(a very important role)': 0,\n",
       " 'Identification/focus(as I have mentioned)': 0,\n",
       " 'Identification/focus(as far as the)': 0,\n",
       " 'Identification/focus(him or her to)': 0,\n",
       " 'Identification/focus(is one of my)': 0,\n",
       " 'Identification/focus(is one of the)': 0,\n",
       " 'Identification/focus(is the most important)': 0,\n",
       " 'Identification/focus(is very important for)': 0,\n",
       " 'Identification/focus(it is very important)': 0,\n",
       " 'Identification/focus(my point of view)': 0,\n",
       " 'Identification/focus(one of the most)': 0,\n",
       " 'Identification/focus(the best way to)': 0,\n",
       " 'Identification/focus(the most important thing)': 0,\n",
       " 'Identification/focus(we can say that)': 0,\n",
       " 'Identification/focus(we can see that)': 0,\n",
       " 'Identification/focus(we can see the)': 0,\n",
       " 'Quantifying': 1,\n",
       " 'Quantifying(a great deal of)': 0,\n",
       " 'Quantifying(a great number of)': 0,\n",
       " 'Quantifying(a large amount of)': 1,\n",
       " 'Quantifying(a lot of people)': 0,\n",
       " 'Quantifying(a lot of problem)': 0,\n",
       " 'Quantifying(a lot of problems)': 0,\n",
       " 'Quantifying(a lot of time)': 0,\n",
       " 'Quantifying(all of them are)': 0,\n",
       " 'Quantifying(and a lot of)': 0,\n",
       " 'Quantifying(bring a lot of)': 0,\n",
       " 'Quantifying(has a lot of)': 0,\n",
       " 'Quantifying(more and more people)': 0,\n",
       " 'Quantifying(most of the people)': 0,\n",
       " 'Quantifying(most of them are)': 0,\n",
       " 'Quantifying(some of them are)': 0,\n",
       " 'Quantifying(that it is more)': 0,\n",
       " 'Quantifying(the rest of the world)': 0,\n",
       " 'Quantifying(the rest of the)': 0,\n",
       " 'Quantifying(there are a lot of)': 0,\n",
       " 'Quantifying(there are many people)': 0,\n",
       " 'Quantifying(there are quite a)': 0,\n",
       " 'Quantifying(there are so many)': 0,\n",
       " 'Quantifying(there are still some)': 0,\n",
       " 'Quantifying(there are too many)': 0,\n",
       " 'Quantifying(with a lot of)': 0,\n",
       " 'Referential': 1,\n",
       " 'Stance': 0,\n",
       " 'Time/place/text deixis': 0,\n",
       " 'Time/place/text deixis(all over the world)': 0,\n",
       " 'Time/place/text deixis(at the beginning of the)': 0,\n",
       " 'Time/place/text deixis(at the same time)': 0,\n",
       " 'Time/place/text deixis(for a long time)': 0,\n",
       " 'Time/place/text deixis(in the following paragraphs)': 0,\n",
       " 'Time/place/text deixis(the end of the)': 0,\n",
       " 'Topic elaboration/clarification': 0,\n",
       " 'Topic elaboration/clarification(and to be a)': 0,\n",
       " 'Topic elaboration/clarification(are more and more)': 0,\n",
       " 'Topic elaboration/clarification(as well as the)': 0,\n",
       " 'Topic elaboration/clarification(but there are still)': 0,\n",
       " 'Topic elaboration/clarification(can be divided into)': 0,\n",
       " 'Topic elaboration/clarification(how to deal with)': 0,\n",
       " 'Topic elaboration/clarification(if you don’t know)': 0,\n",
       " 'Topic elaboration/clarification(if you want to)': 0,\n",
       " 'Topic elaboration/clarification(in order to make)': 0,\n",
       " 'Topic elaboration/clarification(is a kind of)': 0,\n",
       " 'Topic elaboration/clarification(is based on the)': 0,\n",
       " 'Topic elaboration/clarification(is more important than)': 0,\n",
       " 'Topic elaboration/clarification(is totally different from)': 0,\n",
       " 'Topic elaboration/clarification(it is a good)': 0,\n",
       " 'Topic elaboration/clarification(it is a very)': 0,\n",
       " 'Topic elaboration/clarification(it is also a)': 0,\n",
       " 'Topic elaboration/clarification(it is because the)': 0,\n",
       " 'Topic elaboration/clarification(it is not a)': 0,\n",
       " 'Topic elaboration/clarification(on the other hand)': 0,\n",
       " 'Topic elaboration/clarification(there will be a)': 0,\n",
       " 'Topic elaboration/clarification(to cope with the)': 0,\n",
       " 'Topic elaboration/clarification(want to be a)': 0,\n",
       " 'Topic introduction': 0,\n",
       " 'Topic introduction(I am going to)': 0,\n",
       " 'Topic introduction(I would like to)': 0,\n",
       " 'Topic introduction(if there is a)': 0,\n",
       " 'all': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parsing import ParserUDpipe\n",
    "#with open('/Users/irene/Desktop/Диплом/new_data/1.txt', 'r') as file:\n",
    "#    text = file.read()\n",
    "text = 'I have a large amount of fruits.'\n",
    "parser = ParserUDpipe(text)\n",
    "RC = RhetoricalComplexity(text)\n",
    "RC.num_func_ngrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Частотные 4-граммы из REALEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsing import ParserUDpipe\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for root, dirs, files in os.walk('/Users/irene/Downloads/data'):\n",
    "    for name in files:\n",
    "        if name.endswith(\".txt\"):\n",
    "            path = root+'/'+name\n",
    "            paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14359"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afee5487bb14de3894381f649e01173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_freq = {}\n",
    "len_corpus = 0\n",
    "for path in tqdm(paths):\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "    parser = ParserUDpipe(text)\n",
    "    sents, dfs = parser.conllu2df(sentences=True)\n",
    "    for sent in dfs:\n",
    "        tokens = [x.lower() if type(x) == str else x for x in sent['Form']]     \n",
    "        len_corpus += len(tokens)\n",
    "        grams = ngrams(tokens, 4)\n",
    "        for gram in grams:\n",
    "            gram = ' '.join([str(g) for g in gram])\n",
    "            if gram in d_freq:\n",
    "                d_freq[gram] += 1\n",
    "            else:\n",
    "                d_freq[gram] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorted_by_value = sorted(d_freq.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4088807"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494911"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Отобрали с частотностью >= 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import io\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str\n",
    "\n",
    "# Write JSON file\n",
    "data = sorted(d_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "with io.open('sorted_ngrams_REALEC.json', 'w', encoding='utf8') as outfile:\n",
    "    str_ = json.dumps(data,\n",
    "                      indent=4, sort_keys=True,\n",
    "                      separators=(',', ': '), ensure_ascii=False)\n",
    "    outfile.write(to_unicode(str_))\n",
    "\n",
    "# Read JSON file\n",
    "with open('sorted_ngrams_REALEC.json') as data_file:\n",
    "    data_loaded = json.load(data_file)\n",
    "\n",
    "print(data == data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sorted_ngrams_REALEC.json') as data_file:\n",
    "    data_loaded = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to sum up ,', 2306]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('REALECngrams.txt', 'a')\n",
    "for x in data_loaded:\n",
    "    if x[1] >= 200:\n",
    "        file.write(x[0]+'\\n')\n",
    "    else:\n",
    "        break\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('REALECngrams.txt', 'r') as file:\n",
    "    f1 = file.read()\n",
    "with open('ChenBakergrams.txt', 'r') as file:\n",
    "    f2 = file.read()\n",
    "with open('ngrams.txt', 'w') as file:\n",
    "    file.write(f1+'\\n'+f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись списков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('5000frequentCOCA.csv', 'r') as file:\n",
    "#    f = file.read()\n",
    "#f = f.replace('  ', '')\n",
    "#with open('5000frequentCOCA.csv', 'w') as file:\n",
    "#    file.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('5000frequentCOCA.csv')\n",
    "df2 = pd.read_csv('frequentverbsCOCAfrom5000.csv')\n",
    "with open('UWL.txt', 'r') as file:\n",
    "    f = file.read()\n",
    "uwl = f.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import io\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str\n",
    "\n",
    "# Write JSON file\n",
    "data = {'5000frequentCOCA': list(df1['Word']), \n",
    "        'frequentverbsCOCAfrom5000': list(df1['Word']), \n",
    "        'UWL': uwl}\n",
    "with io.open('lists.json', 'w', encoding='utf8') as outfile:\n",
    "    str_ = json.dumps(data,\n",
    "                      indent=4, sort_keys=True,\n",
    "                      separators=(',', ': '), ensure_ascii=False)\n",
    "    outfile.write(to_unicode(str_))\n",
    "\n",
    "# Read JSON file\n",
    "with open('lists.json') as data_file:\n",
    "    data_loaded = json.load(data_file)\n",
    "\n",
    "print(data == data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# проверить ord/lemma/token/verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('5000frequentCOCA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Word</th>\n",
       "      <th>Part_of_speech</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Dispersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>22038615</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>be</td>\n",
       "      <td>v</td>\n",
       "      <td>12545825</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>c</td>\n",
       "      <td>10741073</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>of</td>\n",
       "      <td>i</td>\n",
       "      <td>10343885</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>10144200</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank Word Part_of_speech  Frequency  Dispersion\n",
       "0     1  the              a   22038615        0.98\n",
       "1     2   be              v   12545825        0.97\n",
       "2     3  and              c   10741073        0.99\n",
       "3     4   of              i   10343885        0.97\n",
       "4     5    a              a   10144200        0.98"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
